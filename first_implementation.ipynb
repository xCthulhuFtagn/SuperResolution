{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from skimage.metrics import peak_signal_noise_ratio,  structural_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSRCNN(nn.Module):\n",
    "    def __init__(self, scale_factor, num_channels=1, d=56, s=12, m=4):\n",
    "        super(FSRCNN, self).__init__()\n",
    "\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, d, kernel_size=5, padding=5//2),\n",
    "            nn.PReLU(d)\n",
    "        )\n",
    "\n",
    "        self.shrinking = nn.Sequential(\n",
    "            nn.Conv2d(d, s, kernel_size=1), \n",
    "            nn.PReLU(s)\n",
    "        )\n",
    "\n",
    "        mapping = []\n",
    "        for _ in range(m):\n",
    "            mapping.extend([nn.Conv2d(s, s, kernel_size=3, padding=3//2), nn.PReLU(s)])\n",
    "        self.mapping = nn.Sequential(*mapping)\n",
    "\n",
    "        self.expanding = nn.Sequential(nn.Conv2d(s, d, kernel_size=1), nn.PReLU(d))\n",
    "        \n",
    "        # originally found d instead of s in picture\n",
    "        self.deconvolution = nn.ConvTranspose2d(d, num_channels, kernel_size=9, stride=scale_factor, padding=9//2,\n",
    "                                            output_padding=scale_factor-1)\n",
    "        \n",
    "        # self.out = nn.Sigmoid()\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.feature_extraction:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "        \n",
    "        for m in self.shrinking:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "        \n",
    "        for m in self.mapping:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "        \n",
    "        for m in self.expanding:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "            \n",
    "        nn.init.normal_(self.deconvolution.weight.data, mean=0.0, std=0.001)\n",
    "        nn.init.zeros_(self.deconvolution.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extraction(x)\n",
    "        x = self.shrinking(x)\n",
    "        x = self.mapping(x)\n",
    "        x = self.expanding(x)\n",
    "        x = self.deconvolution(x)\n",
    "        # x = self.out(x) * 255\n",
    "        return (F.tanh(x)).abs() * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = './rutube_hackaton_super_resolution_khabarovsk/train'\n",
    "train_path = './train_frames'\n",
    "\n",
    "lr_path = os.path.join(train_path, 'lr')\n",
    "hr_path = os.path.join(train_path, 'hr')\n",
    "\n",
    "if not os.path.exists(train_path):\n",
    "    os.system(f'mkdir -p {train_path}')\n",
    "\n",
    "if not os.path.exists(lr_path):\n",
    "    os.system(f'mkdir -p {lr_path}')\n",
    "\n",
    "if not os.path.exists(hr_path):\n",
    "    os.system(f'mkdir -p {hr_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(video_path)\n",
    "pairs = []\n",
    "for f in files:\n",
    "    if f.endswith('_144.mp4'):\n",
    "        hr_name = f.split('_')[0] + '_480.mp4'\n",
    "        pairs += [(f, hr_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_frames = 5000\n",
    "# size = int(n_frames // len(pairs))\n",
    "\n",
    "# save_idx = 0\n",
    "# for idx in tqdm(range(len(pairs))):\n",
    "#     pair = pairs[idx]\n",
    "\n",
    "#     lr = os.path.join(video_path, pair[0])\n",
    "#     hr = os.path.join(video_path, pair[1])\n",
    "\n",
    "#     lr_cap = cv2.VideoCapture(lr)\n",
    "#     hr_cap = cv2.VideoCapture(hr)\n",
    "\n",
    "#     lr_len = int(lr_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     hr_len = int(hr_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "#     assert lr_len == hr_len\n",
    "\n",
    "#     frames_idx = [i for i in range(lr_len)]\n",
    "#     if size:\n",
    "#         frames_idx = np.random.choice(frames_idx, size=size, replace=False)\n",
    "\n",
    "#     tmp_idx = 0\n",
    "#     while True:\n",
    "#         success_lr, frame_lr = lr_cap.read()\n",
    "#         success_hr, frame_hr = hr_cap.read()\n",
    "#         if not success_lr or not success_hr:\n",
    "#             break\n",
    "#         if tmp_idx in frames_idx:\n",
    "#             lr_save_path = os.path.join(lr_path, f'{save_idx}.jpg')\n",
    "#             hr_save_path = os.path.join(hr_path, f'{save_idx}.jpg')\n",
    "#             cv2.imwrite(lr_save_path, frame_lr)\n",
    "#             cv2.imwrite(hr_save_path, frame_hr)\n",
    "#             save_idx += 1\n",
    "#         tmp_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    def __init__(self, lr_path, hr_path, transform = None):\n",
    "        self.lr = [os.path.join(lr_path, f) for f in os.listdir(lr_path)]\n",
    "        self.hr = [os.path.join(hr_path, f) for f in os.listdir(hr_path)]\n",
    "        self.lr, self.hr = sorted(self.lr), sorted(self.hr)\n",
    "        assert len(self.lr) == len(self.hr)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr)\n",
    "\n",
    "    def file2np(self, path):\n",
    "        img = cv2.imread(path)\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr = self.file2np(self.lr[idx])\n",
    "        hr = self.file2np(self.hr[idx])\n",
    "        if self.transform is not None: lr, hr = self.transform(lr, hr)\n",
    "        return lr, hr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SameTransform(object):\n",
    "    def __init__(self, mode, crop=None):\n",
    "        self.np2tensor = transforms.ToTensor()\n",
    "        self.mode = mode\n",
    "        self.crop = crop\n",
    "        self.lr_resize = transforms.Resize((120, 214), antialias = True)\n",
    "\n",
    "    def __call__(self, lr, hr):\n",
    "        lr = self.np2tensor(lr)\n",
    "        hr = self.np2tensor(hr)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            lr, hr = self.same_transform(lr, hr)\n",
    "            lr = self.lr_resize(lr)\n",
    "\n",
    "        if self.crop:\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(lr, self.crop)\n",
    "            lr = TF.crop(lr, i, j, h, w)\n",
    "            hr = TF.crop(hr, i, j, h, w)\n",
    "            \n",
    "        return lr, hr#np.expand_dims(lr, 0), np.expand_dims(hr, 0)\n",
    "    \n",
    "    # после преобразований lr и hr сохраняют пространственное соотношение\n",
    "    def same_transform(self, image1, image2, p=0.5):\n",
    "        if random.random() > p:\n",
    "            image1 = TF.hflip(image1)\n",
    "            image2 = TF.hflip(image2)\n",
    "\n",
    "        if random.random() > p:\n",
    "            image1 = TF.vflip(image1)\n",
    "            image2 = TF.vflip(image2)\n",
    "\n",
    "        return image1, image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self):\n",
    "        self.cur_steps = 0\n",
    "        \n",
    "        # устройство на котором идет обучение\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # количество шагов обучения\n",
    "        self.n_steps = 10000\n",
    "\n",
    "        # раз во сколько шагов выводить результаты\n",
    "        self.print_interval = 25\n",
    "        \n",
    "        \n",
    "        # раз во сколько шагов чекпоинт\n",
    "        self.save_interval = 2500\n",
    "\n",
    "        self.batch_size = 50\n",
    "        self.workers = 8\n",
    "\n",
    "        # инициализация модели\n",
    "        self.fsrcnn = FSRCNN(scale_factor=4, num_channels=3).to(self.device)\n",
    "\n",
    "        # конфигурация оптимизатора Adam\n",
    "        self.optimizer = Adam(\n",
    "            self.fsrcnn.parameters(),\n",
    "            0.0001\n",
    "        )\n",
    "\n",
    "        # функция потерь MSE\n",
    "        self.pixel_criterion = nn.MSELoss().to(self.device)\n",
    "\n",
    "        # разрешение hr изображения в формате (h, w)\n",
    "        self.size = (480, 856)\n",
    "        self.gcrop = transforms.CenterCrop([480, 856])\n",
    "\n",
    "        # # аугментации для обучения и валидации\n",
    "        train_transform = SameTransform('train')\n",
    "\n",
    "        # путь где хранятся папки lr и hr с изображениями\n",
    "        train_prefix = './train_frames'\n",
    "\n",
    "        # train датасет\n",
    "        trainset = SRDataset(\n",
    "            f'{train_prefix}/lr',\n",
    "            f'{train_prefix}/hr',\n",
    "            train_transform\n",
    "        )\n",
    "\n",
    "        # даталоадер для обучения батчами\n",
    "        self.trainloader = DataLoader(\n",
    "            trainset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        # аугментации для инференса\n",
    "        self.resize = transforms.Resize(self.size, antialias=None)\n",
    "        self.np2tensor = transforms.ToTensor()\n",
    "        \n",
    "    def read_state(self, state):\n",
    "        self.fsrcnn.load_state_dict(state['model_state_dict'])\n",
    "        self.cur_steps = state['epoch']\n",
    "        self.optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "\n",
    "    def train_step(self, lr, hr):\n",
    "        g_hr = self.fsrcnn(lr)\n",
    "        g_hr = self.gcrop.forward(g_hr)\n",
    "        loss = self.pixel_criterion(g_hr, hr)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self):\n",
    "        self.fsrcnn.train()\n",
    "\n",
    "        while True:\n",
    "            if self.cur_steps >= self.n_steps:\n",
    "                break\n",
    "\n",
    "            for batch in self.trainloader:\n",
    "                lr, hr = batch\n",
    "                lr = lr.to(self.device, non_blocking=True)\n",
    "                hr = hr.to(self.device, non_blocking=True)\n",
    "\n",
    "                mse = self.train_step(lr, hr)\n",
    "                self.cur_steps += 1\n",
    "\n",
    "                if self.cur_steps % self.print_interval == 0:\n",
    "                    print(f'STEP={self.cur_steps} MSE={mse:.5f}')\n",
    "                    \n",
    "                if self.cur_steps % self.save_interval == 0:\n",
    "                    torch.save({\n",
    "                        'epoch': self.cur_steps,\n",
    "                        'model_state_dict': self.fsrcnn.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'loss': mse,\n",
    "                        }, \"./checkpoint\")\n",
    "        torch.save({\n",
    "                'epoch': self.cur_steps,\n",
    "                'model_state_dict': self.fsrcnn.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'loss': mse,\n",
    "                }, \"./checkpoint\")\n",
    "\n",
    "    def frame2tensor(self, img):\n",
    "        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        hr = self.np2tensor(rgb)#self.resize(self.np2tensor(rgb))\n",
    "        return hr\n",
    "\n",
    "    def tensor2frame(self, img):\n",
    "        nparr = (img.detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        nparr = np.transpose(nparr, (1, 2, 0))\n",
    "        bgr = cv2.cvtColor(nparr, cv2.COLOR_RGB2BGR)\n",
    "        return bgr\n",
    "\n",
    "    def super_resolution(self, input_video, output_video, test_video = None):\n",
    "        crop = transforms.CenterCrop(self.size)\n",
    "        self.fsrcnn.eval()\n",
    "\n",
    "        cap = cv2.VideoCapture(input_video)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(\n",
    "            output_video,\n",
    "            fourcc,\n",
    "            fps,\n",
    "            (self.size[1], self.size[0])\n",
    "        )\n",
    "        \n",
    "        resize_lr = transforms.Resize((120, 214), antialias = True)\n",
    "        \n",
    "        if test_video: \n",
    "            test_cap = cv2.VideoCapture(test_video)\n",
    "            psnr = []\n",
    "            ssim = []\n",
    "        \n",
    "        frame = 0\n",
    "\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if test_video: \n",
    "                t_success, test_frame = test_cap.read()\n",
    "                success = success and t_success\n",
    "            \n",
    "            if not success: break\n",
    "            \n",
    "            frame += 1\n",
    "            print(frame)\n",
    "            \n",
    "            tensor = self.frame2tensor(frame).to(self.device).unsqueeze_(0)#lr_crop.forward(self.frame2tensor(frame).to(self.device)).unsqueeze_(0)\n",
    "            tensor = resize_lr(tensor)\n",
    "            with torch.no_grad(): \n",
    "                output_tensor = self.fsrcnn(tensor)\n",
    "            output_frame = self.tensor2frame(crop.forward(output_tensor[0]))\n",
    "\n",
    "            if test_video:\n",
    "                # print(test_frame.shape, output_frame.shape)\n",
    "                psnrs = [peak_signal_noise_ratio(test_frame[:,:,i], output_frame[:,:,i]) for i in range(3)]\n",
    "                psnr.append(np.mean(psnrs))\n",
    "                ssim.append(structural_similarity(test_frame, output_frame, channel_axis = 2))\n",
    "                \n",
    "            writer.write(output_frame)\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()\n",
    "        \n",
    "        if test_video: \n",
    "            test_cap.release()\n",
    "            print(f\"Average PSNR for output video and ground truth is {np.array(psnr).mean()}\")\n",
    "            print(f\"Average SSIM for output video and ground truth is {np.array(ssim).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем объект - trainer для запуска процесса обучения и инференса\n",
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запускаем процесс обучения\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.read_state(torch.load('/home/owner/Documents/DEV/course_work/SuperResolution/checkpoint'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m out_video \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/owner/Documents/DEV/course_work/SuperResolution/1_480_new.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m hr_video \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/owner/Documents/DEV/course_work/SuperResolution/rutube_hackaton_super_resolution_khabarovsk/train/1_480.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuper_resolution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhr_video\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 162\u001b[0m, in \u001b[0;36mTrainer.super_resolution\u001b[0;34m(self, input_video, output_video, test_video)\u001b[0m\n\u001b[1;32m    160\u001b[0m         psnrs \u001b[38;5;241m=\u001b[39m [peak_signal_noise_ratio(test_frame[:,:,i], output_frame[:,:,i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m)]\n\u001b[1;32m    161\u001b[0m         psnr\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(psnrs))\n\u001b[0;32m--> 162\u001b[0m         ssim\u001b[38;5;241m.\u001b[39mappend(\u001b[43mstructural_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel_axis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    164\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwrite(output_frame)\n\u001b[1;32m    166\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Documents/DEV/course_work/SuperResolution/.venv/lib/python3.10/site-packages/skimage/metrics/_structural_similarity.py:133\u001b[0m, in \u001b[0;36mstructural_similarity\u001b[0;34m(im1, im2, win_size, gradient, data_range, channel_axis, gaussian_weights, full, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m _at \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(utils\u001b[38;5;241m.\u001b[39mslice_at_axis, axis\u001b[38;5;241m=\u001b[39mchannel_axis)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nch):\n\u001b[0;32m--> 133\u001b[0m     ch_result \u001b[38;5;241m=\u001b[39m \u001b[43mstructural_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m_at\u001b[49m\u001b[43m(\u001b[49m\u001b[43mch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mim2\u001b[49m\u001b[43m[\u001b[49m\u001b[43m_at\u001b[49m\u001b[43m(\u001b[49m\u001b[43mch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gradient \u001b[38;5;129;01mand\u001b[39;00m full:\n\u001b[1;32m    136\u001b[0m         mssim[ch], G[_at(ch)], S[_at(ch)] \u001b[38;5;241m=\u001b[39m ch_result\n",
      "File \u001b[0;32m~/Documents/DEV/course_work/SuperResolution/.venv/lib/python3.10/site-packages/skimage/metrics/_structural_similarity.py:231\u001b[0m, in \u001b[0;36mstructural_similarity\u001b[0;34m(im1, im2, win_size, gradient, data_range, channel_axis, gaussian_weights, full, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m     cov_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m  \u001b[38;5;66;03m# population covariance to match Wang et. al. 2004\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# compute (weighted) means\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m ux \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfilter_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m uy \u001b[38;5;241m=\u001b[39m filter_func(im2, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfilter_args)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# compute (weighted) variances and covariances\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/DEV/course_work/SuperResolution/.venv/lib/python3.10/site-packages/scipy/ndimage/_filters.py:1092\u001b[0m, in \u001b[0;36muniform_filter\u001b[0;34m(input, size, output, mode, cval, origin, axes)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(axes) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1091\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, size, origin, mode \u001b[38;5;129;01min\u001b[39;00m axes:\n\u001b[0;32m-> 1092\u001b[0m         \u001b[43muniform_filter1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/DEV/course_work/SuperResolution/.venv/lib/python3.10/site-packages/scipy/ndimage/_filters.py:1020\u001b[0m, in \u001b[0;36muniform_filter1d\u001b[0;34m(input, size, axis, output, mode, cval, origin)\u001b[0m\n\u001b[1;32m   1018\u001b[0m mode \u001b[38;5;241m=\u001b[39m _ni_support\u001b[38;5;241m.\u001b[39m_extend_mode_to_code(mode)\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m complex_output:\n\u001b[0;32m-> 1020\u001b[0m     \u001b[43m_nd_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_filter1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m                               \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1023\u001b[0m     _nd_image\u001b[38;5;241m.\u001b[39muniform_filter1d(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mreal, size, axis, output\u001b[38;5;241m.\u001b[39mreal, mode,\n\u001b[1;32m   1024\u001b[0m                                numpy\u001b[38;5;241m.\u001b[39mreal(cval), origin)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr_video = '/home/owner/Documents/DEV/course_work/SuperResolution/rutube_hackaton_super_resolution_khabarovsk/train/1_144.mp4'\n",
    "out_video = '/home/owner/Documents/DEV/course_work/SuperResolution/1_480_new.mp4'\n",
    "hr_video = '/home/owner/Documents/DEV/course_work/SuperResolution/rutube_hackaton_super_resolution_khabarovsk/train/1_480.mp4'\n",
    "\n",
    "\n",
    "trainer.super_resolution(lr_video, out_video, hr_video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
