{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from skimage.metrics import peak_signal_noise_ratio,  structural_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSRCNN(nn.Module):\n",
    "    def __init__(self, scale_factor, num_channels=1, d=56, s=12, m=4):\n",
    "        super(FSRCNN, self).__init__()\n",
    "\n",
    "        self.feature_extraction = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, d, kernel_size=5, padding=5//2),\n",
    "            nn.PReLU(d)\n",
    "        )\n",
    "\n",
    "        self.shrinking = nn.Sequential(\n",
    "            nn.Conv2d(d, s, kernel_size=1), \n",
    "            nn.PReLU(s)\n",
    "        )\n",
    "\n",
    "        mapping = []\n",
    "        for _ in range(m):\n",
    "            mapping.extend([nn.Conv2d(s, s, kernel_size=3, padding=3//2), nn.PReLU(s)])\n",
    "        self.mapping = nn.Sequential(*mapping)\n",
    "\n",
    "        self.expanding = nn.Sequential(nn.Conv2d(s, d, kernel_size=1), nn.PReLU(d))\n",
    "        \n",
    "        # originally found d instead of s in picture\n",
    "        self.deconvolution = nn.ConvTranspose2d(d, num_channels, kernel_size=9, stride=scale_factor, padding=9//2,\n",
    "                                            output_padding=scale_factor-1)\n",
    "        \n",
    "        self.out = nn.Sigmoid()\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.feature_extraction:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "        \n",
    "        for m in self.shrinking:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "        \n",
    "        for m in self.mapping:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "        \n",
    "        for m in self.expanding:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "            \n",
    "        nn.init.normal_(self.deconvolution.weight.data, mean=0.0, std=0.001)\n",
    "        # nn.init.zeros_(self.last_part.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extraction(x)\n",
    "        x = self.shrinking(x)\n",
    "        x = self.mapping(x)\n",
    "        x = self.expanding(x)\n",
    "        x = self.deconvolution(x)\n",
    "        x = self.out(x) * 255\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = './rutube_hackaton_super_resolution_khabarovsk/train'\n",
    "train_path = './train_frames'\n",
    "\n",
    "lr_path = os.path.join(train_path, 'lr')\n",
    "hr_path = os.path.join(train_path, 'hr')\n",
    "\n",
    "if not os.path.exists(train_path):\n",
    "    os.system(f'mkdir -p {train_path}')\n",
    "\n",
    "if not os.path.exists(lr_path):\n",
    "    os.system(f'mkdir -p {lr_path}')\n",
    "\n",
    "if not os.path.exists(hr_path):\n",
    "    os.system(f'mkdir -p {hr_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(video_path)\n",
    "pairs = []\n",
    "for f in files:\n",
    "    if f.endswith('_144.mp4'):\n",
    "        hr_name = f.split('_')[0] + '_480.mp4'\n",
    "        pairs += [(f, hr_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_frames = 5000\n",
    "# size = int(n_frames // len(pairs))\n",
    "\n",
    "# save_idx = 0\n",
    "# for idx in tqdm(range(len(pairs))):\n",
    "#     pair = pairs[idx]\n",
    "\n",
    "#     lr = os.path.join(video_path, pair[0])\n",
    "#     hr = os.path.join(video_path, pair[1])\n",
    "\n",
    "#     lr_cap = cv2.VideoCapture(lr)\n",
    "#     hr_cap = cv2.VideoCapture(hr)\n",
    "\n",
    "#     lr_len = int(lr_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     hr_len = int(hr_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "#     assert lr_len == hr_len\n",
    "\n",
    "#     frames_idx = [i for i in range(lr_len)]\n",
    "#     if size:\n",
    "#         frames_idx = np.random.choice(frames_idx, size=size, replace=False)\n",
    "\n",
    "#     tmp_idx = 0\n",
    "#     while True:\n",
    "#         success_lr, frame_lr = lr_cap.read()\n",
    "#         success_hr, frame_hr = hr_cap.read()\n",
    "#         if not success_lr or not success_hr:\n",
    "#             break\n",
    "#         if tmp_idx in frames_idx:\n",
    "#             lr_save_path = os.path.join(lr_path, f'{save_idx}.jpg')\n",
    "#             hr_save_path = os.path.join(hr_path, f'{save_idx}.jpg')\n",
    "#             cv2.imwrite(lr_save_path, frame_lr)\n",
    "#             cv2.imwrite(hr_save_path, frame_hr)\n",
    "#             save_idx += 1\n",
    "#         tmp_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    def __init__(self, lr_path, hr_path, transform = None):\n",
    "        self.lr = [os.path.join(lr_path, f) for f in os.listdir(lr_path)]\n",
    "        self.hr = [os.path.join(hr_path, f) for f in os.listdir(hr_path)]\n",
    "        self.lr, self.hr = sorted(self.lr), sorted(self.hr)\n",
    "        assert len(self.lr) == len(self.hr)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr)\n",
    "\n",
    "    def file2np(self, path):\n",
    "        img = cv2.imread(path)\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr = self.file2np(self.lr[idx])\n",
    "        hr = self.file2np(self.hr[idx])\n",
    "        if self.transform is not None: lr, hr = self.transform(lr, hr)\n",
    "        return lr, hr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SameTransform(object):\n",
    "    def __init__(self, mode, crop=None):\n",
    "        self.np2tensor = transforms.ToTensor()\n",
    "        self.mode = mode\n",
    "        self.crop = crop\n",
    "        self.lr_resize = transforms.Resize((120, 214), antialias = True)\n",
    "\n",
    "    def __call__(self, lr, hr):\n",
    "        lr = self.np2tensor(lr)\n",
    "        hr = self.np2tensor(hr)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            lr, hr = self.same_transform(lr, hr)\n",
    "            lr = self.lr_resize(lr)\n",
    "\n",
    "        if self.crop:\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(lr, self.crop)\n",
    "            lr = TF.crop(lr, i, j, h, w)\n",
    "            hr = TF.crop(hr, i, j, h, w)\n",
    "            \n",
    "        return lr, hr#np.expand_dims(lr, 0), np.expand_dims(hr, 0)\n",
    "    \n",
    "    # после преобразований lr и hr сохраняют пространственное соотношение\n",
    "    def same_transform(self, image1, image2, p=0.5):\n",
    "        if random.random() > p:\n",
    "            image1 = TF.hflip(image1)\n",
    "            image2 = TF.hflip(image2)\n",
    "\n",
    "        if random.random() > p:\n",
    "            image1 = TF.vflip(image1)\n",
    "            image2 = TF.vflip(image2)\n",
    "\n",
    "        return image1, image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self):\n",
    "        # устройство на котором идет обучение\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # количество шагов обучения\n",
    "        self.n_steps = 10000\n",
    "\n",
    "        # раз во сколько шагов выводить результаты\n",
    "        self.print_interval = 25\n",
    "        \n",
    "        \n",
    "        # раз во сколько шагов чекпоинт\n",
    "        self.save_interval = 2500\n",
    "\n",
    "        self.batch_size = 50\n",
    "        self.workers = 8\n",
    "\n",
    "        # инициализация модели\n",
    "        self.fsrcnn = FSRCNN(scale_factor=4, num_channels=3).to(self.device)\n",
    "\n",
    "        # конфигурация оптимизатора Adam\n",
    "        self.optimizer = Adam(\n",
    "            self.fsrcnn.parameters(),\n",
    "            0.0001\n",
    "        )\n",
    "\n",
    "        # функция потерь MSE\n",
    "        self.pixel_criterion = nn.MSELoss().to(self.device)\n",
    "\n",
    "        # разрешение hr изображения в формате (h, w)\n",
    "        self.size = (480, 856)\n",
    "        self.gcrop = transforms.CenterCrop([480, 856])\n",
    "\n",
    "        # # аугментации для обучения и валидации\n",
    "        train_transform = SameTransform('train')\n",
    "\n",
    "        # путь где хранятся папки lr и hr с изображениями\n",
    "        train_prefix = './train_frames'\n",
    "\n",
    "        # train датасет\n",
    "        trainset = SRDataset(\n",
    "            f'{train_prefix}/lr',\n",
    "            f'{train_prefix}/hr',\n",
    "            train_transform\n",
    "        )\n",
    "\n",
    "        # даталоадер для обучения батчами\n",
    "        self.trainloader = DataLoader(\n",
    "            trainset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        # аугментации для инференса\n",
    "        self.resize = transforms.Resize(self.size, antialias=None)\n",
    "        self.np2tensor = transforms.ToTensor()\n",
    "\n",
    "    def train_step(self, lr, hr):\n",
    "        g_hr = self.fsrcnn(lr)\n",
    "        g_hr = self.gcrop.forward(g_hr)\n",
    "        loss = self.pixel_criterion(g_hr, hr)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self):\n",
    "        self.fsrcnn.train()\n",
    "        step = 0\n",
    "\n",
    "        while True:\n",
    "            if step >= self.n_steps:\n",
    "                break\n",
    "\n",
    "            for batch in self.trainloader:\n",
    "                lr, hr = batch\n",
    "                lr = lr.to(self.device, non_blocking=True)\n",
    "                hr = hr.to(self.device, non_blocking=True)\n",
    "\n",
    "                mse = self.train_step(lr, hr)\n",
    "                step += 1\n",
    "\n",
    "                if step % self.print_interval == 0:\n",
    "                    print(f'STEP={step} MSE={mse:.5f}')\n",
    "                    \n",
    "                if step % self.save_interval == 0:\n",
    "                    torch.save({\n",
    "                        'epoch': step,\n",
    "                        'model_state_dict': self.fsrcnn.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'loss': mse,\n",
    "                        }, \"./checkpoint\")\n",
    "        torch.save({\n",
    "                'epoch': step,\n",
    "                'model_state_dict': self.fsrcnn.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'loss': mse,\n",
    "                }, \"./checkpoint\")\n",
    "\n",
    "    def frame2tensor(self, img):\n",
    "        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        hr = self.np2tensor(rgb)#self.resize(self.np2tensor(rgb))\n",
    "        return hr\n",
    "\n",
    "    def tensor2frame(self, img):\n",
    "        nparr = (img.detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        nparr = np.transpose(nparr, (1, 2, 0))\n",
    "        bgr = cv2.cvtColor(nparr, cv2.COLOR_RGB2BGR)\n",
    "        return bgr\n",
    "\n",
    "    def super_resolution(self, input_video, output_video, test_video = None):\n",
    "        crop = transforms.CenterCrop(self.size)\n",
    "        self.fsrcnn.eval()\n",
    "\n",
    "        cap = cv2.VideoCapture(input_video)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(\n",
    "            output_video,\n",
    "            fourcc,\n",
    "            fps,\n",
    "            (self.size[1], self.size[0])\n",
    "        )\n",
    "        \n",
    "        resize_lr = transforms.Resize((120, 214), antialias = True)\n",
    "        \n",
    "        if test_video: \n",
    "            test_cap = cv2.VideoCapture(test_video)\n",
    "            psnr = []\n",
    "            ssim = []\n",
    "\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if test_video: \n",
    "                t_success, test_frame = test_cap.read()\n",
    "                success = success and t_success\n",
    "            \n",
    "            if not success: break\n",
    "            \n",
    "            if test_video:\n",
    "                psnr.append(peak_signal_noise_ratio(test_frame, frame))\n",
    "                ssim.append(structural_similarity(test_frame, frame))\n",
    "            \n",
    "            tensor = self.frame2tensor(frame).to(self.device).unsqueeze_(0)#lr_crop.forward(self.frame2tensor(frame).to(self.device)).unsqueeze_(0)\n",
    "            tensor = resize_lr(tensor)\n",
    "            with torch.no_grad(): \n",
    "                output_tensor = self.fsrcnn(tensor)\n",
    "            output_frame = self.tensor2frame(crop.forward(output_tensor[0]))\n",
    "\n",
    "            writer.write(output_frame)\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()\n",
    "        \n",
    "        if test_video: \n",
    "            test_cap.release()\n",
    "            print(f\"Average PSNR for output video and ground truth is {np.array(psnr).mean()}\")\n",
    "            print(f\"Average SSIM for output video and ground truth is {np.array(ssim).mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем объект - trainer для запуска процесса обучения и инференса\n",
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP=25 MSE=13369.08691\n",
      "STEP=50 MSE=4490.10889\n",
      "STEP=75 MSE=1117.88025\n",
      "STEP=100 MSE=536.88116\n",
      "STEP=125 MSE=452.49768\n",
      "STEP=150 MSE=184.67203\n",
      "STEP=175 MSE=180.98439\n",
      "STEP=200 MSE=150.79094\n",
      "STEP=225 MSE=68.04465\n",
      "STEP=250 MSE=84.89474\n",
      "STEP=275 MSE=85.66084\n",
      "STEP=300 MSE=43.99466\n",
      "STEP=325 MSE=55.67273\n",
      "STEP=350 MSE=28.66559\n",
      "STEP=375 MSE=33.55159\n",
      "STEP=400 MSE=31.96677\n",
      "STEP=425 MSE=24.69353\n",
      "STEP=450 MSE=19.99586\n",
      "STEP=475 MSE=14.54031\n",
      "STEP=500 MSE=27.95485\n",
      "STEP=525 MSE=14.71871\n",
      "STEP=550 MSE=23.31065\n",
      "STEP=575 MSE=13.05248\n",
      "STEP=600 MSE=10.56397\n",
      "STEP=625 MSE=12.19871\n",
      "STEP=650 MSE=7.06052\n",
      "STEP=675 MSE=10.53920\n",
      "STEP=700 MSE=6.03039\n",
      "STEP=725 MSE=11.15774\n",
      "STEP=750 MSE=7.13961\n",
      "STEP=775 MSE=9.42714\n",
      "STEP=800 MSE=9.89115\n",
      "STEP=825 MSE=6.27144\n",
      "STEP=850 MSE=7.40824\n",
      "STEP=875 MSE=6.28052\n",
      "STEP=900 MSE=3.05488\n",
      "STEP=925 MSE=4.55955\n",
      "STEP=950 MSE=3.97193\n",
      "STEP=975 MSE=4.14490\n",
      "STEP=1000 MSE=6.59838\n",
      "STEP=1025 MSE=4.46128\n",
      "STEP=1050 MSE=2.80200\n",
      "STEP=1075 MSE=4.08943\n",
      "STEP=1100 MSE=5.47512\n",
      "STEP=1125 MSE=3.97100\n",
      "STEP=1150 MSE=2.51728\n",
      "STEP=1175 MSE=3.22427\n",
      "STEP=1200 MSE=2.80735\n",
      "STEP=1225 MSE=2.88146\n",
      "STEP=1250 MSE=1.16729\n",
      "STEP=1275 MSE=2.91367\n",
      "STEP=1300 MSE=3.10448\n",
      "STEP=1325 MSE=3.71983\n",
      "STEP=1350 MSE=1.90291\n",
      "STEP=1375 MSE=2.90275\n",
      "STEP=1400 MSE=3.68460\n",
      "STEP=1425 MSE=2.85610\n",
      "STEP=1450 MSE=2.25391\n",
      "STEP=1475 MSE=3.08740\n",
      "STEP=1500 MSE=1.73941\n",
      "STEP=1525 MSE=1.35623\n",
      "STEP=1550 MSE=1.90091\n",
      "STEP=1575 MSE=1.74497\n",
      "STEP=1600 MSE=1.88886\n",
      "STEP=1625 MSE=1.46713\n",
      "STEP=1650 MSE=1.66837\n",
      "STEP=1675 MSE=1.35604\n",
      "STEP=1700 MSE=1.16578\n",
      "STEP=1725 MSE=1.27249\n",
      "STEP=1750 MSE=1.43328\n",
      "STEP=1775 MSE=1.28683\n",
      "STEP=1800 MSE=1.58947\n",
      "STEP=1825 MSE=1.16545\n",
      "STEP=1850 MSE=1.48920\n",
      "STEP=1875 MSE=1.72892\n",
      "STEP=1900 MSE=1.02717\n",
      "STEP=1925 MSE=0.69247\n",
      "STEP=1950 MSE=0.73691\n",
      "STEP=1975 MSE=1.43520\n",
      "STEP=2000 MSE=0.94535\n",
      "STEP=2025 MSE=1.29332\n",
      "STEP=2050 MSE=1.08970\n",
      "STEP=2075 MSE=1.26642\n",
      "STEP=2100 MSE=0.59195\n",
      "STEP=2125 MSE=1.00996\n",
      "STEP=2150 MSE=1.08116\n",
      "STEP=2175 MSE=0.70780\n",
      "STEP=2200 MSE=1.22772\n",
      "STEP=2225 MSE=0.77479\n",
      "STEP=2250 MSE=0.67259\n",
      "STEP=2275 MSE=0.89457\n",
      "STEP=2300 MSE=0.62394\n",
      "STEP=2325 MSE=0.64305\n",
      "STEP=2350 MSE=0.90855\n",
      "STEP=2375 MSE=0.51244\n",
      "STEP=2400 MSE=0.90510\n",
      "STEP=2425 MSE=0.88738\n",
      "STEP=2450 MSE=0.44363\n",
      "STEP=2475 MSE=0.61702\n",
      "STEP=2500 MSE=0.45765\n",
      "STEP=2525 MSE=0.53727\n",
      "STEP=2550 MSE=0.62892\n",
      "STEP=2575 MSE=0.56124\n",
      "STEP=2600 MSE=0.93793\n",
      "STEP=2625 MSE=0.48102\n",
      "STEP=2650 MSE=0.53471\n",
      "STEP=2675 MSE=0.44533\n",
      "STEP=2700 MSE=0.48262\n",
      "STEP=2725 MSE=0.57324\n",
      "STEP=2750 MSE=0.51379\n",
      "STEP=2775 MSE=0.50519\n",
      "STEP=2800 MSE=0.46974\n",
      "STEP=2825 MSE=0.75707\n",
      "STEP=2850 MSE=0.67375\n",
      "STEP=2875 MSE=0.44466\n",
      "STEP=2900 MSE=0.49603\n",
      "STEP=2925 MSE=0.47217\n",
      "STEP=2950 MSE=0.58603\n",
      "STEP=2975 MSE=0.72388\n",
      "STEP=3000 MSE=0.54202\n",
      "STEP=3025 MSE=0.53510\n",
      "STEP=3050 MSE=0.27722\n",
      "STEP=3075 MSE=0.33779\n",
      "STEP=3100 MSE=0.47731\n",
      "STEP=3125 MSE=0.33971\n",
      "STEP=3150 MSE=0.28077\n",
      "STEP=3175 MSE=0.36210\n",
      "STEP=3200 MSE=0.48159\n",
      "STEP=3225 MSE=0.32944\n",
      "STEP=3250 MSE=0.31634\n",
      "STEP=3275 MSE=0.46113\n",
      "STEP=3300 MSE=0.42236\n",
      "STEP=3325 MSE=0.34722\n",
      "STEP=3350 MSE=0.31829\n",
      "STEP=3375 MSE=0.46196\n",
      "STEP=3400 MSE=0.37084\n",
      "STEP=3425 MSE=0.28959\n",
      "STEP=3450 MSE=0.28067\n",
      "STEP=3475 MSE=0.29011\n",
      "STEP=3500 MSE=0.28560\n",
      "STEP=3525 MSE=0.27166\n",
      "STEP=3550 MSE=0.35208\n",
      "STEP=3575 MSE=0.36055\n",
      "STEP=3600 MSE=0.27924\n",
      "STEP=3625 MSE=0.36684\n",
      "STEP=3650 MSE=0.33531\n",
      "STEP=3675 MSE=0.29336\n",
      "STEP=3700 MSE=0.46454\n",
      "STEP=3725 MSE=0.28787\n",
      "STEP=3750 MSE=0.41641\n",
      "STEP=3775 MSE=0.34193\n",
      "STEP=3800 MSE=0.38173\n",
      "STEP=3825 MSE=0.28304\n",
      "STEP=3850 MSE=0.27996\n",
      "STEP=3875 MSE=0.32046\n",
      "STEP=3900 MSE=0.26082\n",
      "STEP=3925 MSE=0.32081\n",
      "STEP=3950 MSE=0.32045\n",
      "STEP=3975 MSE=0.27177\n",
      "STEP=4000 MSE=0.31821\n",
      "STEP=4025 MSE=0.31459\n",
      "STEP=4050 MSE=0.24632\n",
      "STEP=4075 MSE=0.29262\n",
      "STEP=4100 MSE=0.27004\n",
      "STEP=4125 MSE=0.21220\n",
      "STEP=4150 MSE=0.23714\n",
      "STEP=4175 MSE=0.27693\n",
      "STEP=4200 MSE=0.22836\n",
      "STEP=4225 MSE=0.25955\n",
      "STEP=4250 MSE=0.21293\n",
      "STEP=4275 MSE=0.20960\n",
      "STEP=4300 MSE=0.24110\n",
      "STEP=4325 MSE=0.25152\n",
      "STEP=4350 MSE=0.26820\n",
      "STEP=4375 MSE=0.22888\n",
      "STEP=4400 MSE=0.22630\n",
      "STEP=4425 MSE=0.19236\n",
      "STEP=4450 MSE=0.20529\n",
      "STEP=4475 MSE=0.26930\n",
      "STEP=4500 MSE=0.19891\n",
      "STEP=4525 MSE=0.25174\n",
      "STEP=4550 MSE=0.26119\n",
      "STEP=4575 MSE=0.27075\n",
      "STEP=4600 MSE=0.23247\n",
      "STEP=4625 MSE=0.21723\n",
      "STEP=4650 MSE=0.19766\n",
      "STEP=4675 MSE=0.25503\n",
      "STEP=4700 MSE=0.25429\n",
      "STEP=4725 MSE=0.23863\n",
      "STEP=4750 MSE=0.28114\n",
      "STEP=4775 MSE=0.24171\n",
      "STEP=4800 MSE=0.19079\n",
      "STEP=4825 MSE=0.22717\n",
      "STEP=4850 MSE=0.24871\n",
      "STEP=4875 MSE=0.24098\n",
      "STEP=4900 MSE=0.24954\n",
      "STEP=4925 MSE=0.22712\n",
      "STEP=4950 MSE=0.18951\n",
      "STEP=4975 MSE=0.28007\n",
      "STEP=5000 MSE=0.23177\n",
      "STEP=5025 MSE=0.23856\n",
      "STEP=5050 MSE=0.21027\n",
      "STEP=5075 MSE=0.19084\n",
      "STEP=5100 MSE=0.22842\n",
      "STEP=5125 MSE=0.24804\n",
      "STEP=5150 MSE=0.19604\n",
      "STEP=5175 MSE=0.18948\n",
      "STEP=5200 MSE=0.21098\n",
      "STEP=5225 MSE=0.25233\n",
      "STEP=5250 MSE=0.23362\n",
      "STEP=5275 MSE=0.18440\n",
      "STEP=5300 MSE=0.21784\n",
      "STEP=5325 MSE=0.20703\n",
      "STEP=5350 MSE=0.24191\n",
      "STEP=5375 MSE=0.19112\n",
      "STEP=5400 MSE=0.21671\n",
      "STEP=5425 MSE=0.23913\n",
      "STEP=5450 MSE=0.19261\n",
      "STEP=5475 MSE=0.20173\n",
      "STEP=5500 MSE=0.21362\n",
      "STEP=5525 MSE=0.18999\n",
      "STEP=5550 MSE=0.22496\n",
      "STEP=5575 MSE=0.17997\n",
      "STEP=5600 MSE=0.20334\n",
      "STEP=5625 MSE=0.20716\n",
      "STEP=5650 MSE=0.21704\n",
      "STEP=5675 MSE=0.23529\n",
      "STEP=5700 MSE=0.17481\n",
      "STEP=5725 MSE=0.21264\n",
      "STEP=5750 MSE=0.20581\n",
      "STEP=5775 MSE=0.21576\n",
      "STEP=5800 MSE=0.20469\n",
      "STEP=5825 MSE=0.18720\n",
      "STEP=5850 MSE=0.20570\n",
      "STEP=5875 MSE=0.20341\n",
      "STEP=5900 MSE=0.19719\n",
      "STEP=5925 MSE=0.20090\n",
      "STEP=5950 MSE=0.17713\n",
      "STEP=5975 MSE=0.19181\n",
      "STEP=6000 MSE=0.21023\n",
      "STEP=6025 MSE=0.20190\n",
      "STEP=6050 MSE=0.20353\n",
      "STEP=6075 MSE=0.19272\n",
      "STEP=6100 MSE=0.20818\n",
      "STEP=6125 MSE=0.19897\n",
      "STEP=6150 MSE=0.17893\n",
      "STEP=6175 MSE=0.19474\n",
      "STEP=6200 MSE=0.19179\n",
      "STEP=6225 MSE=0.19475\n",
      "STEP=6250 MSE=0.16848\n",
      "STEP=6275 MSE=0.21531\n",
      "STEP=6300 MSE=0.22188\n",
      "STEP=6325 MSE=0.19541\n",
      "STEP=6350 MSE=0.22107\n",
      "STEP=6375 MSE=0.20775\n",
      "STEP=6400 MSE=0.17052\n",
      "STEP=6425 MSE=0.17821\n",
      "STEP=6450 MSE=0.21837\n",
      "STEP=6475 MSE=0.22250\n",
      "STEP=6500 MSE=0.18857\n",
      "STEP=6525 MSE=0.19374\n",
      "STEP=6550 MSE=0.18466\n",
      "STEP=6575 MSE=0.19773\n",
      "STEP=6600 MSE=0.15594\n",
      "STEP=6625 MSE=0.19719\n",
      "STEP=6650 MSE=0.20300\n",
      "STEP=6675 MSE=0.19629\n",
      "STEP=6700 MSE=0.16245\n",
      "STEP=6725 MSE=0.20178\n",
      "STEP=6750 MSE=0.21010\n",
      "STEP=6775 MSE=0.20834\n",
      "STEP=6800 MSE=0.19479\n",
      "STEP=6825 MSE=0.18670\n",
      "STEP=6850 MSE=0.20704\n",
      "STEP=6875 MSE=0.20105\n",
      "STEP=6900 MSE=0.18068\n",
      "STEP=6925 MSE=0.18444\n",
      "STEP=6950 MSE=0.23259\n",
      "STEP=6975 MSE=0.19040\n",
      "STEP=7000 MSE=0.19264\n",
      "STEP=7025 MSE=0.18169\n",
      "STEP=7050 MSE=0.21150\n",
      "STEP=7075 MSE=0.18687\n",
      "STEP=7100 MSE=0.17146\n",
      "STEP=7125 MSE=0.18844\n",
      "STEP=7150 MSE=0.18630\n",
      "STEP=7175 MSE=0.18081\n",
      "STEP=7200 MSE=0.17756\n",
      "STEP=7225 MSE=0.17748\n",
      "STEP=7250 MSE=0.19878\n",
      "STEP=7275 MSE=0.17205\n",
      "STEP=7300 MSE=0.19634\n",
      "STEP=7325 MSE=0.18076\n",
      "STEP=7350 MSE=0.20158\n",
      "STEP=7375 MSE=0.19243\n",
      "STEP=7400 MSE=0.16503\n",
      "STEP=7425 MSE=0.17639\n",
      "STEP=7450 MSE=0.18828\n",
      "STEP=7475 MSE=0.19143\n",
      "STEP=7500 MSE=0.18525\n",
      "STEP=7525 MSE=0.21802\n",
      "STEP=7550 MSE=0.17672\n",
      "STEP=7575 MSE=0.19400\n",
      "STEP=7600 MSE=0.17120\n",
      "STEP=7625 MSE=0.20844\n",
      "STEP=7650 MSE=0.15536\n",
      "STEP=7675 MSE=0.20535\n",
      "STEP=7700 MSE=0.17672\n",
      "STEP=7725 MSE=0.18482\n",
      "STEP=7750 MSE=0.21436\n",
      "STEP=7775 MSE=0.21018\n",
      "STEP=7800 MSE=0.18601\n",
      "STEP=7825 MSE=0.20962\n",
      "STEP=7850 MSE=0.17293\n",
      "STEP=7875 MSE=0.15918\n",
      "STEP=7900 MSE=0.20207\n",
      "STEP=7925 MSE=0.20469\n",
      "STEP=7950 MSE=0.19154\n",
      "STEP=7975 MSE=0.18647\n",
      "STEP=8000 MSE=0.18642\n",
      "STEP=8025 MSE=0.19917\n",
      "STEP=8050 MSE=0.21817\n",
      "STEP=8075 MSE=0.19590\n",
      "STEP=8100 MSE=0.20100\n",
      "STEP=8125 MSE=0.18542\n",
      "STEP=8150 MSE=0.16576\n",
      "STEP=8175 MSE=0.19771\n",
      "STEP=8200 MSE=0.17519\n",
      "STEP=8225 MSE=0.15067\n",
      "STEP=8250 MSE=0.19327\n",
      "STEP=8275 MSE=0.18141\n",
      "STEP=8300 MSE=0.17451\n",
      "STEP=8325 MSE=0.18903\n",
      "STEP=8350 MSE=0.18437\n",
      "STEP=8375 MSE=0.17083\n",
      "STEP=8400 MSE=0.18455\n",
      "STEP=8425 MSE=0.14555\n",
      "STEP=8450 MSE=0.16823\n",
      "STEP=8475 MSE=0.19068\n",
      "STEP=8500 MSE=0.18281\n",
      "STEP=8525 MSE=0.19491\n",
      "STEP=8550 MSE=0.18155\n",
      "STEP=8575 MSE=0.21393\n",
      "STEP=8600 MSE=0.18589\n",
      "STEP=8625 MSE=0.16977\n",
      "STEP=8650 MSE=0.18044\n",
      "STEP=8675 MSE=0.17456\n",
      "STEP=8700 MSE=0.20075\n",
      "STEP=8725 MSE=0.18528\n",
      "STEP=8750 MSE=0.18994\n",
      "STEP=8775 MSE=0.17136\n",
      "STEP=8800 MSE=0.20676\n",
      "STEP=8825 MSE=0.19858\n",
      "STEP=8850 MSE=0.18799\n",
      "STEP=8875 MSE=0.17541\n",
      "STEP=8900 MSE=0.19232\n",
      "STEP=8925 MSE=0.16200\n",
      "STEP=8950 MSE=0.20376\n",
      "STEP=8975 MSE=0.18891\n",
      "STEP=9000 MSE=0.18900\n",
      "STEP=9025 MSE=0.19219\n",
      "STEP=9050 MSE=0.16700\n",
      "STEP=9075 MSE=0.20561\n",
      "STEP=9100 MSE=0.19735\n",
      "STEP=9125 MSE=0.16594\n",
      "STEP=9150 MSE=0.18646\n",
      "STEP=9175 MSE=0.17079\n",
      "STEP=9200 MSE=0.17499\n",
      "STEP=9225 MSE=0.18375\n",
      "STEP=9250 MSE=0.18924\n",
      "STEP=9275 MSE=0.19398\n",
      "STEP=9300 MSE=0.17032\n",
      "STEP=9325 MSE=0.19362\n",
      "STEP=9350 MSE=0.18760\n",
      "STEP=9375 MSE=0.17562\n",
      "STEP=9400 MSE=0.16618\n",
      "STEP=9425 MSE=0.19295\n",
      "STEP=9450 MSE=0.17738\n",
      "STEP=9475 MSE=0.17957\n",
      "STEP=9500 MSE=0.18967\n",
      "STEP=9525 MSE=0.18656\n",
      "STEP=9550 MSE=0.19203\n",
      "STEP=9575 MSE=0.18171\n",
      "STEP=9600 MSE=0.16646\n",
      "STEP=9625 MSE=0.16851\n",
      "STEP=9650 MSE=0.15054\n",
      "STEP=9675 MSE=0.16350\n",
      "STEP=9700 MSE=0.18977\n",
      "STEP=9725 MSE=0.17282\n",
      "STEP=9750 MSE=0.18884\n",
      "STEP=9775 MSE=0.18376\n",
      "STEP=9800 MSE=0.21325\n",
      "STEP=9825 MSE=0.17580\n",
      "STEP=9850 MSE=0.20057\n",
      "STEP=9875 MSE=0.18240\n",
      "STEP=9900 MSE=0.16061\n",
      "STEP=9925 MSE=0.19062\n",
      "STEP=9950 MSE=0.18578\n",
      "STEP=9975 MSE=0.17723\n",
      "STEP=10000 MSE=0.17601\n"
     ]
    }
   ],
   "source": [
    "# запускаем процесс обучения\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PSNR for output video and ground truth is nan\n",
      "Average SSIM for output video and ground truth is nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4540/133851742.py:162: RuntimeWarning: Mean of empty slice.\n",
      "  print(f\"Average PSNR for output video and ground truth is {np.array(psnr).mean()}\")\n",
      "/home/owner/Documents/DEV/course_work/SuperResolution/.venv/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/tmp/ipykernel_4540/133851742.py:163: RuntimeWarning: Mean of empty slice.\n",
      "  print(f\"Average SSIM for output video and ground truth is {np.array(ssim).mean()}\")\n"
     ]
    }
   ],
   "source": [
    "lr_video = '/home/owner/Documents/DEV/Python/SuperResolution/rutube_hackaton_super_resolution_khabarovsk/train/1_144.mp4'\n",
    "out_video = '/home/owner/Documents/DEV/Python/SuperResolution/1_480_new.mp4'\n",
    "hr_video = '/home/owner/Documents/DEV/Python/SuperResolution/rutube_hackaton_super_resolution_khabarovsk/train/1_480.mp4'\n",
    "\n",
    "\n",
    "trainer.super_resolution(lr_video, out_video, hr_video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
