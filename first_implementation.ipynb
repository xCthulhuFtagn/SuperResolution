{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSRCNN(nn.Module):\n",
    "    def __init__(self, scale_factor, num_channels=1, d=56, s=12, m=4):\n",
    "        super(FSRCNN, self).__init__()\n",
    "        # feature extraction\n",
    "        self.first_part = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, d, kernel_size=5, padding=5//2),\n",
    "            nn.PReLU(d)\n",
    "        )\n",
    "        # shrinking\n",
    "        self.mid_part = [nn.Conv2d(d, s, kernel_size=1), nn.PReLU(s)]\n",
    "        # mapping\n",
    "        for _ in range(m):\n",
    "            self.mid_part.extend([nn.Conv2d(s, s, kernel_size=3, padding=3//2), nn.PReLU(s)])\n",
    "        # expanding\n",
    "        self.mid_part.extend([nn.Conv2d(s, d, kernel_size=1), nn.PReLU(d)])\n",
    "        self.mid_part = nn.Sequential(*self.mid_part)\n",
    "        # Deconvolution\n",
    "        # originally found d instead of s in picture\n",
    "        self.last_part = nn.ConvTranspose2d(d, num_channels, kernel_size=9, stride=scale_factor, padding=9//2,\n",
    "                                            output_padding=scale_factor-1)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.first_part:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "        for m in self.mid_part:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "        nn.init.normal_(self.last_part.weight.data, mean=0.0, std=0.001)\n",
    "        nn.init.zeros_(self.last_part.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_part(x)\n",
    "        x = self.mid_part(x)\n",
    "        x = self.last_part(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_path = './rutube_hackaton_super_resolution_khabarovsk/train'\n",
    "# train_path = './train_frames'\n",
    "\n",
    "# lr_path = os.path.join(train_path, 'lr')\n",
    "# hr_path = os.path.join(train_path, 'hr')\n",
    "\n",
    "# if not os.path.exists(train_path):\n",
    "#     os.system(f'mkdir -p {train_path}')\n",
    "\n",
    "# if not os.path.exists(lr_path):\n",
    "#     os.system(f'mkdir -p {lr_path}')\n",
    "\n",
    "# if not os.path.exists(hr_path):\n",
    "#     os.system(f'mkdir -p {hr_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = os.listdir(video_path)\n",
    "# pairs = []\n",
    "# for f in files:\n",
    "#     if f.endswith('_144.mp4'):\n",
    "#         hr_name = f.split('_')[0] + '_480.mp4'\n",
    "#         pairs += [(f, hr_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_frames = 5000\n",
    "# size = int(n_frames // len(pairs))\n",
    "\n",
    "# save_idx = 0\n",
    "# for idx in tqdm(range(len(pairs))):\n",
    "#     pair = pairs[idx]\n",
    "\n",
    "#     lr = os.path.join(video_path, pair[0])\n",
    "#     hr = os.path.join(video_path, pair[1])\n",
    "\n",
    "#     lr_cap = cv2.VideoCapture(lr)\n",
    "#     hr_cap = cv2.VideoCapture(hr)\n",
    "\n",
    "#     lr_len = int(lr_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     hr_len = int(hr_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "#     assert lr_len == hr_len\n",
    "\n",
    "#     frames_idx = [i for i in range(lr_len)]\n",
    "#     if size:\n",
    "#         frames_idx = np.random.choice(frames_idx, size=size, replace=False)\n",
    "\n",
    "#     tmp_idx = 0\n",
    "#     while True:\n",
    "#         success_lr, frame_lr = lr_cap.read()\n",
    "#         success_hr, frame_hr = hr_cap.read()\n",
    "#         if not success_lr or not success_hr:\n",
    "#             break\n",
    "#         if tmp_idx in frames_idx:\n",
    "#             lr_save_path = os.path.join(lr_path, f'{save_idx}.jpg')\n",
    "#             hr_save_path = os.path.join(hr_path, f'{save_idx}.jpg')\n",
    "#             cv2.imwrite(lr_save_path, frame_lr)\n",
    "#             cv2.imwrite(hr_save_path, frame_hr)\n",
    "#             save_idx += 1\n",
    "#         tmp_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    def __init__(self, lr_path, hr_path, transform = None):\n",
    "        self.lr = [os.path.join(lr_path, f) for f in os.listdir(lr_path)]\n",
    "        self.hr = [os.path.join(hr_path, f) for f in os.listdir(hr_path)]\n",
    "        self.lr, self.hr = sorted(self.lr), sorted(self.hr)\n",
    "        assert len(self.lr) == len(self.hr)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr)\n",
    "\n",
    "    def file2np(self, path):\n",
    "        img = cv2.imread(path)\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr = self.file2np(self.lr[idx])\n",
    "        hr = self.file2np(self.hr[idx])\n",
    "        if self.transform is not None: lr, hr = self.transform(lr, hr)\n",
    "        return lr, hr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SameTransform(object):\n",
    "    def __init__(self, mode, crop=None):\n",
    "        self.np2tensor = transforms.ToTensor()\n",
    "        self.mode = mode\n",
    "        self.crop = crop\n",
    "        self.lr_resize = transforms.Resize((120, 214), antialias = True)\n",
    "\n",
    "    def __call__(self, lr, hr):\n",
    "        lr = self.np2tensor(lr)#self.lr_crop.forward(self.np2tensor(lr))\n",
    "        hr = self.np2tensor(hr)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            lr, hr = self.same_transform(lr, hr)\n",
    "            lr = self.lr_resize(lr)\n",
    "\n",
    "        if self.crop:\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(lr, self.crop)\n",
    "            lr = TF.crop(lr, i, j, h, w)\n",
    "            hr = TF.crop(hr, i, j, h, w)\n",
    "            \n",
    "        return lr, hr#np.expand_dims(lr, 0), np.expand_dims(hr, 0)\n",
    "    \n",
    "    # после преобразований lr и hr сохраняют пространственное соотношение\n",
    "    def same_transform(self, image1, image2, p=0.5):\n",
    "        if random.random() > p:\n",
    "            image1 = TF.hflip(image1)\n",
    "            image2 = TF.hflip(image2)\n",
    "\n",
    "        if random.random() > p:\n",
    "            image1 = TF.vflip(image1)\n",
    "            image2 = TF.vflip(image2)\n",
    "\n",
    "        return image1, image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self):\n",
    "        # устройство для обучения\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # количество шагов обучения\n",
    "        self.n_steps = 10000\n",
    "\n",
    "        # раз в сколько шагов выводить результаты\n",
    "        self.print_interval = 25\n",
    "\n",
    "        # раз в сколько шагов сохранять чекпоинт\n",
    "        self.save_interval = 2500\n",
    "\n",
    "        self.batch_size =50\n",
    "        self.workers = 8\n",
    "\n",
    "        # инициализация модели\n",
    "        self.fsrcnn = FSRCNN(scale_factor=4, num_channels=3).to(self.device)\n",
    "\n",
    "        # конфигурация оптимизатора Adam\n",
    "        self.optimizer = Adam(\n",
    "            self.fsrcnn.parameters(),\n",
    "            0.0001\n",
    "        )\n",
    "\n",
    "        # функция потерь MSE\n",
    "        self.pixel_criterion = nn.MSELoss().to(self.device)\n",
    "\n",
    "        # разрешение hr изображения в формате (h, w)\n",
    "        self.size = (480, 856)\n",
    "        self.gcrop = transforms.CenterCrop([480, 856])\n",
    "\n",
    "        # # аугментации для обучения и валидации\n",
    "        train_transform = SameTransform('train')\n",
    "\n",
    "        # путь где хранятся папки lr и hr с изображениями\n",
    "        train_prefix = './train_frames'\n",
    "\n",
    "        # train датасет\n",
    "        trainset = SRDataset(\n",
    "            f'{train_prefix}/lr',\n",
    "            f'{train_prefix}/hr',\n",
    "            train_transform\n",
    "        )\n",
    "\n",
    "        # даталоадер для обучения батчами\n",
    "        self.trainloader = DataLoader(\n",
    "            trainset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        # аугментации для инференса\n",
    "        self.resize = transforms.Resize(self.size, antialias=None)\n",
    "        self.np2tensor = transforms.ToTensor()\n",
    "\n",
    "    def train_step(self, lr, hr):\n",
    "        g_hr = self.fsrcnn(lr)\n",
    "        g_hr = self.gcrop.forward(g_hr)\n",
    "        loss = self.pixel_criterion(g_hr, hr)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self):\n",
    "        self.fsrcnn.train()\n",
    "        step = 0\n",
    "\n",
    "        while True:\n",
    "            if step >= self.n_steps:\n",
    "                break\n",
    "\n",
    "            for batch in self.trainloader:\n",
    "                lr, hr = batch\n",
    "                lr = lr.to(self.device, non_blocking=True)\n",
    "                hr = hr.to(self.device, non_blocking=True)\n",
    "\n",
    "                mse = self.train_step(lr, hr)\n",
    "                step += 1\n",
    "\n",
    "                if step % self.print_interval == 0:\n",
    "                    print(f'STEP={step} MSE={mse:.5f}')\n",
    "\n",
    "    def frame2tensor(self, img):\n",
    "        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        hr = self.np2tensor(rgb)#self.resize(self.np2tensor(rgb))\n",
    "        return hr\n",
    "\n",
    "    def tensor2frame(self, img):\n",
    "        nparr = (img.detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        nparr = np.transpose(nparr, (1, 2, 0))\n",
    "        bgr = cv2.cvtColor(nparr, cv2.COLOR_RGB2BGR)\n",
    "        return bgr\n",
    "\n",
    "\n",
    "    def super_resolution(self, input_video, output_video):\n",
    "        crop = transforms.CenterCrop(self.size)\n",
    "        self.fsrcnn.eval()\n",
    "\n",
    "        cap = cv2.VideoCapture(input_video)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(\n",
    "            output_video,\n",
    "            fourcc,\n",
    "            fps,\n",
    "            (self.size[1], self.size[0])\n",
    "        )\n",
    "        \n",
    "        resize_lr = transforms.Resize((120, 214), antialias = True)\n",
    "\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            tensor = self.frame2tensor(frame).to(self.device).unsqueeze_(0)#lr_crop.forward(self.frame2tensor(frame).to(self.device)).unsqueeze_(0)\n",
    "            tensor = resize_lr(tensor)\n",
    "            with torch.no_grad():\n",
    "                output_tensor = self.fsrcnn(tensor)\n",
    "            output_frame = self.tensor2frame(crop.forward(output_tensor[0]))\n",
    "\n",
    "            writer.write(output_frame)\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем объект - trainer для запуска процесса обучения и инференса\n",
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP=25 MSE=0.02402\n",
      "STEP=50 MSE=0.01589\n",
      "STEP=75 MSE=0.01272\n",
      "STEP=100 MSE=0.01634\n",
      "STEP=125 MSE=0.01326\n",
      "STEP=150 MSE=0.00938\n",
      "STEP=175 MSE=0.00904\n",
      "STEP=200 MSE=0.01084\n",
      "STEP=225 MSE=0.00815\n",
      "STEP=250 MSE=0.01073\n",
      "STEP=275 MSE=0.01715\n",
      "STEP=300 MSE=0.01079\n",
      "STEP=325 MSE=0.01100\n",
      "STEP=350 MSE=0.00561\n",
      "STEP=375 MSE=0.00637\n",
      "STEP=400 MSE=0.01522\n",
      "STEP=425 MSE=0.00750\n",
      "STEP=450 MSE=0.00498\n",
      "STEP=475 MSE=0.00610\n",
      "STEP=500 MSE=0.00491\n",
      "STEP=525 MSE=0.00478\n",
      "STEP=550 MSE=0.00513\n",
      "STEP=575 MSE=0.00492\n",
      "STEP=600 MSE=0.00539\n",
      "STEP=625 MSE=0.00628\n",
      "STEP=650 MSE=0.00416\n",
      "STEP=675 MSE=0.00399\n",
      "STEP=700 MSE=0.00441\n",
      "STEP=725 MSE=0.00382\n",
      "STEP=750 MSE=0.00393\n",
      "STEP=775 MSE=0.00422\n",
      "STEP=800 MSE=0.00509\n",
      "STEP=825 MSE=0.00425\n",
      "STEP=850 MSE=0.00478\n",
      "STEP=875 MSE=0.00459\n",
      "STEP=900 MSE=0.00436\n",
      "STEP=925 MSE=0.00418\n",
      "STEP=950 MSE=0.00440\n",
      "STEP=975 MSE=0.00324\n",
      "STEP=1000 MSE=0.00603\n",
      "STEP=1025 MSE=0.00182\n",
      "STEP=1050 MSE=0.00380\n",
      "STEP=1075 MSE=0.00429\n",
      "STEP=1100 MSE=0.00332\n",
      "STEP=1125 MSE=0.00276\n",
      "STEP=1150 MSE=0.00296\n",
      "STEP=1175 MSE=0.00534\n",
      "STEP=1200 MSE=0.00388\n",
      "STEP=1225 MSE=0.00259\n",
      "STEP=1250 MSE=0.00321\n",
      "STEP=1275 MSE=0.00329\n",
      "STEP=1300 MSE=0.00254\n",
      "STEP=1325 MSE=0.00244\n",
      "STEP=1350 MSE=0.00334\n",
      "STEP=1375 MSE=0.00480\n",
      "STEP=1400 MSE=0.00247\n",
      "STEP=1425 MSE=0.00217\n",
      "STEP=1450 MSE=0.00199\n",
      "STEP=1475 MSE=0.00232\n",
      "STEP=1500 MSE=0.00183\n",
      "STEP=1525 MSE=0.00249\n",
      "STEP=1550 MSE=0.00243\n",
      "STEP=1575 MSE=0.00235\n",
      "STEP=1600 MSE=0.00209\n",
      "STEP=1625 MSE=0.00152\n",
      "STEP=1650 MSE=0.00262\n",
      "STEP=1675 MSE=0.00171\n",
      "STEP=1700 MSE=0.00222\n",
      "STEP=1725 MSE=0.00166\n",
      "STEP=1750 MSE=0.00191\n",
      "STEP=1775 MSE=0.00174\n",
      "STEP=1800 MSE=0.00305\n",
      "STEP=1825 MSE=0.00217\n",
      "STEP=1850 MSE=0.00160\n",
      "STEP=1875 MSE=0.00264\n",
      "STEP=1900 MSE=0.00228\n",
      "STEP=1925 MSE=0.00261\n",
      "STEP=1950 MSE=0.00278\n",
      "STEP=1975 MSE=0.00289\n",
      "STEP=2000 MSE=0.00189\n",
      "STEP=2025 MSE=0.00214\n",
      "STEP=2050 MSE=0.00185\n",
      "STEP=2075 MSE=0.00267\n",
      "STEP=2100 MSE=0.00204\n",
      "STEP=2125 MSE=0.00223\n",
      "STEP=2150 MSE=0.00164\n",
      "STEP=2175 MSE=0.00250\n",
      "STEP=2200 MSE=0.00212\n",
      "STEP=2225 MSE=0.00293\n",
      "STEP=2250 MSE=0.00206\n",
      "STEP=2275 MSE=0.00279\n",
      "STEP=2300 MSE=0.00281\n",
      "STEP=2325 MSE=0.00219\n",
      "STEP=2350 MSE=0.00199\n",
      "STEP=2375 MSE=0.00148\n",
      "STEP=2400 MSE=0.00369\n",
      "STEP=2425 MSE=0.00139\n",
      "STEP=2450 MSE=0.00257\n",
      "STEP=2475 MSE=0.00307\n",
      "STEP=2500 MSE=0.00263\n",
      "STEP=2525 MSE=0.00134\n",
      "STEP=2550 MSE=0.00214\n",
      "STEP=2575 MSE=0.00203\n",
      "STEP=2600 MSE=0.00211\n",
      "STEP=2625 MSE=0.00195\n",
      "STEP=2650 MSE=0.00212\n",
      "STEP=2675 MSE=0.00275\n",
      "STEP=2700 MSE=0.00209\n",
      "STEP=2725 MSE=0.00216\n",
      "STEP=2750 MSE=0.00202\n",
      "STEP=2775 MSE=0.00238\n",
      "STEP=2800 MSE=0.00199\n",
      "STEP=2825 MSE=0.00169\n",
      "STEP=2850 MSE=0.00155\n",
      "STEP=2875 MSE=0.00156\n",
      "STEP=2900 MSE=0.00228\n",
      "STEP=2925 MSE=0.00183\n",
      "STEP=2950 MSE=0.00251\n",
      "STEP=2975 MSE=0.00233\n",
      "STEP=3000 MSE=0.00186\n",
      "STEP=3025 MSE=0.00247\n",
      "STEP=3050 MSE=0.00150\n",
      "STEP=3075 MSE=0.00193\n",
      "STEP=3100 MSE=0.00192\n",
      "STEP=3125 MSE=0.00315\n",
      "STEP=3150 MSE=0.00161\n",
      "STEP=3175 MSE=0.00187\n",
      "STEP=3200 MSE=0.00178\n",
      "STEP=3225 MSE=0.00212\n",
      "STEP=3250 MSE=0.00168\n",
      "STEP=3275 MSE=0.00183\n",
      "STEP=3300 MSE=0.00187\n",
      "STEP=3325 MSE=0.00232\n",
      "STEP=3350 MSE=0.00105\n",
      "STEP=3375 MSE=0.00283\n",
      "STEP=3400 MSE=0.00156\n",
      "STEP=3425 MSE=0.00182\n",
      "STEP=3450 MSE=0.00301\n",
      "STEP=3475 MSE=0.00169\n",
      "STEP=3500 MSE=0.00149\n",
      "STEP=3525 MSE=0.00145\n",
      "STEP=3550 MSE=0.00134\n",
      "STEP=3575 MSE=0.00174\n",
      "STEP=3600 MSE=0.00310\n",
      "STEP=3625 MSE=0.00161\n",
      "STEP=3650 MSE=0.00173\n",
      "STEP=3675 MSE=0.00237\n",
      "STEP=3700 MSE=0.00202\n",
      "STEP=3725 MSE=0.00171\n",
      "STEP=3750 MSE=0.00192\n",
      "STEP=3775 MSE=0.00160\n",
      "STEP=3800 MSE=0.00224\n",
      "STEP=3825 MSE=0.00120\n",
      "STEP=3850 MSE=0.00168\n",
      "STEP=3875 MSE=0.00094\n",
      "STEP=3900 MSE=0.00229\n",
      "STEP=3925 MSE=0.00245\n",
      "STEP=3950 MSE=0.00391\n",
      "STEP=3975 MSE=0.00164\n",
      "STEP=4000 MSE=0.00155\n",
      "STEP=4025 MSE=0.00181\n",
      "STEP=4050 MSE=0.00129\n",
      "STEP=4075 MSE=0.00234\n",
      "STEP=4100 MSE=0.00195\n",
      "STEP=4125 MSE=0.00142\n",
      "STEP=4150 MSE=0.00201\n",
      "STEP=4175 MSE=0.00157\n",
      "STEP=4200 MSE=0.00281\n",
      "STEP=4225 MSE=0.00175\n",
      "STEP=4250 MSE=0.00166\n",
      "STEP=4275 MSE=0.00193\n",
      "STEP=4300 MSE=0.00207\n",
      "STEP=4325 MSE=0.00136\n",
      "STEP=4350 MSE=0.00154\n",
      "STEP=4375 MSE=0.00144\n",
      "STEP=4400 MSE=0.00137\n",
      "STEP=4425 MSE=0.00213\n",
      "STEP=4450 MSE=0.00168\n",
      "STEP=4475 MSE=0.00247\n",
      "STEP=4500 MSE=0.00138\n",
      "STEP=4525 MSE=0.00170\n",
      "STEP=4550 MSE=0.00135\n",
      "STEP=4575 MSE=0.00207\n",
      "STEP=4600 MSE=0.00196\n",
      "STEP=4625 MSE=0.00161\n",
      "STEP=4650 MSE=0.00168\n",
      "STEP=4675 MSE=0.00211\n",
      "STEP=4700 MSE=0.00172\n",
      "STEP=4725 MSE=0.00178\n",
      "STEP=4750 MSE=0.00124\n",
      "STEP=4775 MSE=0.00187\n",
      "STEP=4800 MSE=0.00149\n",
      "STEP=4825 MSE=0.00166\n",
      "STEP=4850 MSE=0.00150\n",
      "STEP=4875 MSE=0.00167\n",
      "STEP=4900 MSE=0.00213\n",
      "STEP=4925 MSE=0.00151\n",
      "STEP=4950 MSE=0.00101\n",
      "STEP=4975 MSE=0.00204\n",
      "STEP=5000 MSE=0.00202\n",
      "STEP=5025 MSE=0.00173\n",
      "STEP=5050 MSE=0.00224\n",
      "STEP=5075 MSE=0.00196\n",
      "STEP=5100 MSE=0.00155\n",
      "STEP=5125 MSE=0.00136\n",
      "STEP=5150 MSE=0.00195\n",
      "STEP=5175 MSE=0.00202\n",
      "STEP=5200 MSE=0.00148\n",
      "STEP=5225 MSE=0.00130\n",
      "STEP=5250 MSE=0.00131\n",
      "STEP=5275 MSE=0.00139\n",
      "STEP=5300 MSE=0.00162\n",
      "STEP=5325 MSE=0.00181\n",
      "STEP=5350 MSE=0.00225\n",
      "STEP=5375 MSE=0.00147\n",
      "STEP=5400 MSE=0.00214\n",
      "STEP=5425 MSE=0.00144\n",
      "STEP=5450 MSE=0.00208\n",
      "STEP=5475 MSE=0.00153\n",
      "STEP=5500 MSE=0.00183\n",
      "STEP=5525 MSE=0.00111\n",
      "STEP=5550 MSE=0.00150\n",
      "STEP=5575 MSE=0.00165\n",
      "STEP=5600 MSE=0.00197\n",
      "STEP=5625 MSE=0.00136\n",
      "STEP=5650 MSE=0.00168\n",
      "STEP=5675 MSE=0.00210\n",
      "STEP=5700 MSE=0.00132\n",
      "STEP=5725 MSE=0.00124\n",
      "STEP=5750 MSE=0.00184\n",
      "STEP=5775 MSE=0.00184\n",
      "STEP=5800 MSE=0.00151\n",
      "STEP=5825 MSE=0.00100\n",
      "STEP=5850 MSE=0.00223\n",
      "STEP=5875 MSE=0.00119\n",
      "STEP=5900 MSE=0.00138\n",
      "STEP=5925 MSE=0.00161\n",
      "STEP=5950 MSE=0.00159\n",
      "STEP=5975 MSE=0.00155\n",
      "STEP=6000 MSE=0.00142\n",
      "STEP=6025 MSE=0.00135\n",
      "STEP=6050 MSE=0.00200\n",
      "STEP=6075 MSE=0.00143\n",
      "STEP=6100 MSE=0.00171\n",
      "STEP=6125 MSE=0.00166\n",
      "STEP=6150 MSE=0.00119\n",
      "STEP=6175 MSE=0.00221\n",
      "STEP=6200 MSE=0.00202\n",
      "STEP=6225 MSE=0.00137\n",
      "STEP=6250 MSE=0.00226\n",
      "STEP=6275 MSE=0.00167\n",
      "STEP=6300 MSE=0.00167\n",
      "STEP=6325 MSE=0.00154\n",
      "STEP=6350 MSE=0.00168\n",
      "STEP=6375 MSE=0.00125\n",
      "STEP=6400 MSE=0.00303\n",
      "STEP=6425 MSE=0.00202\n",
      "STEP=6450 MSE=0.00138\n",
      "STEP=6475 MSE=0.00158\n",
      "STEP=6500 MSE=0.00150\n",
      "STEP=6525 MSE=0.00154\n",
      "STEP=6550 MSE=0.00282\n",
      "STEP=6575 MSE=0.00187\n",
      "STEP=6600 MSE=0.00206\n",
      "STEP=6625 MSE=0.00179\n",
      "STEP=6650 MSE=0.00090\n",
      "STEP=6675 MSE=0.00159\n",
      "STEP=6700 MSE=0.00172\n",
      "STEP=6725 MSE=0.00147\n",
      "STEP=6750 MSE=0.00200\n",
      "STEP=6775 MSE=0.00203\n",
      "STEP=6800 MSE=0.00182\n",
      "STEP=6825 MSE=0.00167\n",
      "STEP=6850 MSE=0.00178\n",
      "STEP=6875 MSE=0.00254\n",
      "STEP=6900 MSE=0.00224\n",
      "STEP=6925 MSE=0.00144\n",
      "STEP=6950 MSE=0.00309\n",
      "STEP=6975 MSE=0.00119\n",
      "STEP=7000 MSE=0.00206\n",
      "STEP=7025 MSE=0.00122\n",
      "STEP=7050 MSE=0.00170\n",
      "STEP=7075 MSE=0.00133\n",
      "STEP=7100 MSE=0.00206\n",
      "STEP=7125 MSE=0.00129\n",
      "STEP=7150 MSE=0.00242\n",
      "STEP=7175 MSE=0.00167\n",
      "STEP=7200 MSE=0.00194\n",
      "STEP=7225 MSE=0.00195\n",
      "STEP=7250 MSE=0.00242\n",
      "STEP=7275 MSE=0.00121\n",
      "STEP=7300 MSE=0.00165\n",
      "STEP=7325 MSE=0.00162\n",
      "STEP=7350 MSE=0.00217\n",
      "STEP=7375 MSE=0.00200\n",
      "STEP=7400 MSE=0.00132\n",
      "STEP=7425 MSE=0.00136\n",
      "STEP=7450 MSE=0.00159\n",
      "STEP=7475 MSE=0.00183\n",
      "STEP=7500 MSE=0.00187\n",
      "STEP=7525 MSE=0.00147\n",
      "STEP=7550 MSE=0.00179\n",
      "STEP=7575 MSE=0.00157\n",
      "STEP=7600 MSE=0.00186\n",
      "STEP=7625 MSE=0.00167\n",
      "STEP=7650 MSE=0.00145\n",
      "STEP=7675 MSE=0.00266\n",
      "STEP=7700 MSE=0.00165\n",
      "STEP=7725 MSE=0.00177\n",
      "STEP=7750 MSE=0.00201\n",
      "STEP=7775 MSE=0.00242\n",
      "STEP=7800 MSE=0.00162\n",
      "STEP=7825 MSE=0.00122\n",
      "STEP=7850 MSE=0.00165\n",
      "STEP=7875 MSE=0.00182\n",
      "STEP=7900 MSE=0.00153\n",
      "STEP=7925 MSE=0.00166\n",
      "STEP=7950 MSE=0.00170\n",
      "STEP=7975 MSE=0.00207\n",
      "STEP=8000 MSE=0.00138\n",
      "STEP=8025 MSE=0.00187\n",
      "STEP=8050 MSE=0.00184\n",
      "STEP=8075 MSE=0.00195\n",
      "STEP=8100 MSE=0.00191\n",
      "STEP=8125 MSE=0.00158\n",
      "STEP=8150 MSE=0.00141\n",
      "STEP=8175 MSE=0.00120\n",
      "STEP=8200 MSE=0.00141\n",
      "STEP=8225 MSE=0.00157\n",
      "STEP=8250 MSE=0.00167\n",
      "STEP=8275 MSE=0.00147\n",
      "STEP=8300 MSE=0.00190\n",
      "STEP=8325 MSE=0.00130\n",
      "STEP=8350 MSE=0.00163\n",
      "STEP=8375 MSE=0.00118\n",
      "STEP=8400 MSE=0.00235\n",
      "STEP=8425 MSE=0.00156\n",
      "STEP=8450 MSE=0.00170\n",
      "STEP=8475 MSE=0.00185\n",
      "STEP=8500 MSE=0.00254\n",
      "STEP=8525 MSE=0.00202\n",
      "STEP=8550 MSE=0.00137\n",
      "STEP=8575 MSE=0.00212\n",
      "STEP=8600 MSE=0.00171\n",
      "STEP=8625 MSE=0.00143\n",
      "STEP=8650 MSE=0.00182\n",
      "STEP=8675 MSE=0.00135\n",
      "STEP=8700 MSE=0.00232\n",
      "STEP=8725 MSE=0.00150\n",
      "STEP=8750 MSE=0.00134\n",
      "STEP=8775 MSE=0.00163\n",
      "STEP=8800 MSE=0.00117\n",
      "STEP=8825 MSE=0.00218\n",
      "STEP=8850 MSE=0.00198\n",
      "STEP=8875 MSE=0.00196\n",
      "STEP=8900 MSE=0.00171\n",
      "STEP=8925 MSE=0.00153\n",
      "STEP=8950 MSE=0.00195\n",
      "STEP=8975 MSE=0.00264\n",
      "STEP=9000 MSE=0.00140\n",
      "STEP=9025 MSE=0.00127\n",
      "STEP=9050 MSE=0.00234\n",
      "STEP=9075 MSE=0.00167\n",
      "STEP=9100 MSE=0.00142\n",
      "STEP=9125 MSE=0.00240\n",
      "STEP=9150 MSE=0.00167\n",
      "STEP=9175 MSE=0.00229\n",
      "STEP=9200 MSE=0.00342\n",
      "STEP=9225 MSE=0.00259\n",
      "STEP=9250 MSE=0.00146\n",
      "STEP=9275 MSE=0.00147\n",
      "STEP=9300 MSE=0.00188\n",
      "STEP=9325 MSE=0.00128\n",
      "STEP=9350 MSE=0.00106\n",
      "STEP=9375 MSE=0.00170\n",
      "STEP=9400 MSE=0.00187\n",
      "STEP=9425 MSE=0.00135\n",
      "STEP=9450 MSE=0.00093\n",
      "STEP=9475 MSE=0.00163\n",
      "STEP=9500 MSE=0.00174\n",
      "STEP=9525 MSE=0.00151\n",
      "STEP=9550 MSE=0.00162\n",
      "STEP=9575 MSE=0.00133\n",
      "STEP=9600 MSE=0.00123\n",
      "STEP=9625 MSE=0.00178\n",
      "STEP=9650 MSE=0.00155\n",
      "STEP=9675 MSE=0.00122\n",
      "STEP=9700 MSE=0.00146\n",
      "STEP=9725 MSE=0.00194\n",
      "STEP=9750 MSE=0.00174\n",
      "STEP=9775 MSE=0.00183\n",
      "STEP=9800 MSE=0.00221\n",
      "STEP=9825 MSE=0.00138\n",
      "STEP=9850 MSE=0.00153\n",
      "STEP=9875 MSE=0.00122\n",
      "STEP=9900 MSE=0.00149\n",
      "STEP=9925 MSE=0.00174\n",
      "STEP=9950 MSE=0.00187\n",
      "STEP=9975 MSE=0.00183\n",
      "STEP=10000 MSE=0.00116\n",
      "STEP=10025 MSE=0.00192\n",
      "STEP=10050 MSE=0.00230\n",
      "STEP=10075 MSE=0.00168\n",
      "STEP=10100 MSE=0.00150\n",
      "STEP=10125 MSE=0.00144\n",
      "STEP=10150 MSE=0.00125\n",
      "STEP=10175 MSE=0.00120\n",
      "STEP=10200 MSE=0.00185\n",
      "STEP=10225 MSE=0.00199\n",
      "STEP=10250 MSE=0.00152\n",
      "STEP=10275 MSE=0.00169\n",
      "STEP=10300 MSE=0.00140\n",
      "STEP=10325 MSE=0.00161\n",
      "STEP=10350 MSE=0.00220\n",
      "STEP=10375 MSE=0.00105\n",
      "STEP=10400 MSE=0.00253\n",
      "STEP=10425 MSE=0.00134\n",
      "STEP=10450 MSE=0.00145\n",
      "STEP=10475 MSE=0.00202\n",
      "STEP=10500 MSE=0.00161\n",
      "STEP=10525 MSE=0.00140\n",
      "STEP=10550 MSE=0.00132\n",
      "STEP=10575 MSE=0.00140\n",
      "STEP=10600 MSE=0.00134\n",
      "STEP=10625 MSE=0.00122\n",
      "STEP=10650 MSE=0.00228\n",
      "STEP=10675 MSE=0.00121\n",
      "STEP=10700 MSE=0.00186\n",
      "STEP=10725 MSE=0.00123\n",
      "STEP=10750 MSE=0.00208\n",
      "STEP=10775 MSE=0.00167\n",
      "STEP=10800 MSE=0.00241\n",
      "STEP=10825 MSE=0.00164\n",
      "STEP=10850 MSE=0.00129\n",
      "STEP=10875 MSE=0.00136\n",
      "STEP=10900 MSE=0.00172\n",
      "STEP=10925 MSE=0.00205\n",
      "STEP=10950 MSE=0.00140\n",
      "STEP=10975 MSE=0.00181\n",
      "STEP=11000 MSE=0.00199\n",
      "STEP=11025 MSE=0.00160\n",
      "STEP=11050 MSE=0.00177\n",
      "STEP=11075 MSE=0.00176\n",
      "STEP=11100 MSE=0.00120\n",
      "STEP=11125 MSE=0.00233\n",
      "STEP=11150 MSE=0.00142\n",
      "STEP=11175 MSE=0.00120\n",
      "STEP=11200 MSE=0.00163\n",
      "STEP=11225 MSE=0.00155\n",
      "STEP=11250 MSE=0.00186\n",
      "STEP=11275 MSE=0.00279\n",
      "STEP=11300 MSE=0.00218\n",
      "STEP=11325 MSE=0.00156\n",
      "STEP=11350 MSE=0.00127\n",
      "STEP=11375 MSE=0.00252\n",
      "STEP=11400 MSE=0.00165\n",
      "STEP=11425 MSE=0.00132\n",
      "STEP=11450 MSE=0.00090\n",
      "STEP=11475 MSE=0.00137\n",
      "STEP=11500 MSE=0.00162\n",
      "STEP=11525 MSE=0.00122\n",
      "STEP=11550 MSE=0.00167\n",
      "STEP=11575 MSE=0.00114\n",
      "STEP=11600 MSE=0.00205\n",
      "STEP=11625 MSE=0.00178\n",
      "STEP=11650 MSE=0.00079\n",
      "STEP=11675 MSE=0.00164\n",
      "STEP=11700 MSE=0.00142\n",
      "STEP=11725 MSE=0.00146\n",
      "STEP=11750 MSE=0.00198\n",
      "STEP=11775 MSE=0.00130\n",
      "STEP=11800 MSE=0.00153\n",
      "STEP=11825 MSE=0.00168\n",
      "STEP=11850 MSE=0.00169\n",
      "STEP=11875 MSE=0.00231\n",
      "STEP=11900 MSE=0.00155\n",
      "STEP=11925 MSE=0.00131\n",
      "STEP=11950 MSE=0.00187\n",
      "STEP=11975 MSE=0.00173\n",
      "STEP=12000 MSE=0.00137\n",
      "STEP=12025 MSE=0.00158\n",
      "STEP=12050 MSE=0.00117\n",
      "STEP=12075 MSE=0.00130\n",
      "STEP=12100 MSE=0.00148\n",
      "STEP=12125 MSE=0.00153\n",
      "STEP=12150 MSE=0.00133\n",
      "STEP=12175 MSE=0.00184\n",
      "STEP=12200 MSE=0.00197\n",
      "STEP=12225 MSE=0.00116\n",
      "STEP=12250 MSE=0.00112\n",
      "STEP=12275 MSE=0.00162\n",
      "STEP=12300 MSE=0.00135\n",
      "STEP=12325 MSE=0.00178\n",
      "STEP=12350 MSE=0.00122\n",
      "STEP=12375 MSE=0.00184\n",
      "STEP=12400 MSE=0.00126\n",
      "STEP=12425 MSE=0.00143\n",
      "STEP=12450 MSE=0.00099\n",
      "STEP=12475 MSE=0.00155\n",
      "STEP=12500 MSE=0.00134\n",
      "STEP=12525 MSE=0.00190\n",
      "STEP=12550 MSE=0.00184\n",
      "STEP=12575 MSE=0.00118\n",
      "STEP=12600 MSE=0.00132\n",
      "STEP=12625 MSE=0.00119\n",
      "STEP=12650 MSE=0.00142\n",
      "STEP=12675 MSE=0.00105\n",
      "STEP=12700 MSE=0.00148\n",
      "STEP=12725 MSE=0.00215\n",
      "STEP=12750 MSE=0.00131\n",
      "STEP=12775 MSE=0.00187\n",
      "STEP=12800 MSE=0.00114\n",
      "STEP=12825 MSE=0.00126\n",
      "STEP=12850 MSE=0.00138\n",
      "STEP=12875 MSE=0.00164\n",
      "STEP=12900 MSE=0.00126\n",
      "STEP=12925 MSE=0.00168\n",
      "STEP=12950 MSE=0.00184\n",
      "STEP=12975 MSE=0.00147\n",
      "STEP=13000 MSE=0.00126\n",
      "STEP=13025 MSE=0.00165\n",
      "STEP=13050 MSE=0.00186\n",
      "STEP=13075 MSE=0.00128\n",
      "STEP=13100 MSE=0.00204\n",
      "STEP=13125 MSE=0.00151\n",
      "STEP=13150 MSE=0.00128\n",
      "STEP=13175 MSE=0.00141\n",
      "STEP=13200 MSE=0.00117\n",
      "STEP=13225 MSE=0.00249\n",
      "STEP=13250 MSE=0.00143\n",
      "STEP=13275 MSE=0.00183\n",
      "STEP=13300 MSE=0.00236\n",
      "STEP=13325 MSE=0.00166\n",
      "STEP=13350 MSE=0.00130\n",
      "STEP=13375 MSE=0.00159\n",
      "STEP=13400 MSE=0.00107\n",
      "STEP=13425 MSE=0.00122\n",
      "STEP=13450 MSE=0.00202\n",
      "STEP=13475 MSE=0.00198\n",
      "STEP=13500 MSE=0.00189\n",
      "STEP=13525 MSE=0.00242\n",
      "STEP=13550 MSE=0.00232\n",
      "STEP=13575 MSE=0.00125\n",
      "STEP=13600 MSE=0.00147\n",
      "STEP=13625 MSE=0.00108\n",
      "STEP=13650 MSE=0.00198\n",
      "STEP=13675 MSE=0.00109\n",
      "STEP=13700 MSE=0.00244\n",
      "STEP=13725 MSE=0.00156\n",
      "STEP=13750 MSE=0.00096\n",
      "STEP=13775 MSE=0.00160\n",
      "STEP=13800 MSE=0.00140\n",
      "STEP=13825 MSE=0.00162\n",
      "STEP=13850 MSE=0.00169\n",
      "STEP=13875 MSE=0.00136\n",
      "STEP=13900 MSE=0.00139\n",
      "STEP=13925 MSE=0.00112\n",
      "STEP=13950 MSE=0.00146\n",
      "STEP=13975 MSE=0.00129\n",
      "STEP=14000 MSE=0.00194\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# запускаем процесс обучения\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[81], line 83\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m lr \u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     81\u001b[0m hr \u001b[38;5;241m=\u001b[39m hr\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 83\u001b[0m mse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[81], line 68\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self, lr, hr)\u001b[0m\n\u001b[1;32m     66\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# запускаем процесс обучения\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_video = '/home/owner/Documents/DEV/Python/SuperResolution/rutube_hackaton_super_resolution_khabarovsk/train/1_144.mp4'\n",
    "hr_video = '/home/owner/Documents/DEV/Python/SuperResolution/rutube_hackaton_super_resolution_khabarovsk/train/1_480_new.mp4'\n",
    "\n",
    "trainer.super_resolution(lr_video, hr_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "unpickling stack underflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# torch.save(trainer.fsrcnn, 'model-40-20000.pt')\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfsrcnn \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/owner/Documents/DEV/Python/SuperResolution/output.avi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DEV/Python/SuperResolution/.venv/lib/python3.10/site-packages/torch/serialization.py:1028\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1027\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1028\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DEV/Python/SuperResolution/.venv/lib/python3.10/site-packages/torch/serialization.py:1246\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1243\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived object of type \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctionality.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1246\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m \u001b[43mpickle_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid magic number; corrupt file?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: unpickling stack underflow"
     ]
    }
   ],
   "source": [
    "# torch.save(trainer.fsrcnn, 'model-40-20000.pt')\n",
    "# trainer.fsrcnn = torch.load('/home/owner/Documents/DEV/Python/SuperResolution/output.avi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
