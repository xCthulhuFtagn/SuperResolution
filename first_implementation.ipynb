{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSRCNN(nn.Module):\n",
    "    def __init__(self, scale_factor, num_channels=1, d=56, s=12, m=4):\n",
    "        super(FSRCNN, self).__init__()\n",
    "        # feature extraction\n",
    "        self.first_part = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, d, kernel_size=5, padding=5//2),\n",
    "            nn.PReLU(d)\n",
    "        )\n",
    "        # shrinking\n",
    "        self.mid_part = [nn.Conv2d(d, s, kernel_size=1), nn.PReLU(s)]\n",
    "        # mapping\n",
    "        for _ in range(m):\n",
    "            self.mid_part.extend([nn.Conv2d(s, s, kernel_size=3, padding=3//2), nn.PReLU(s)])\n",
    "        # expanding\n",
    "        self.mid_part.extend([nn.Conv2d(s, d, kernel_size=1), nn.PReLU(d)])\n",
    "        self.mid_part = nn.Sequential(*self.mid_part)\n",
    "        # Deconvolution\n",
    "        self.last_part = nn.ConvTranspose2d(d, num_channels, kernel_size=9, stride=scale_factor, padding=9//2,\n",
    "                                            output_padding=scale_factor-1)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.first_part:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "        for m in self.mid_part:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "        nn.init.normal_(self.last_part.weight.data, mean=0.0, std=0.001)\n",
    "        nn.init.zeros_(self.last_part.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_part(x)\n",
    "        x = self.mid_part(x)\n",
    "        x = self.last_part(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self):\n",
    "        # устройство для обучения\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # количество шагов обучения\n",
    "        self.n_steps = 10000\n",
    "\n",
    "        # раз в сколько шагов выводить результаты\n",
    "        self.print_interval = 25\n",
    "\n",
    "        # раз в сколько шагов сохранять чекпоинт\n",
    "        self.save_interval = 2500\n",
    "\n",
    "        self.batch_size = 24\n",
    "        self.workers = 8\n",
    "\n",
    "        # инициализация модели\n",
    "        self.fsrcnn = FSRCNN().to(self.device)\n",
    "\n",
    "        # конфигурация оптимизатора Adam\n",
    "        self.optimizer = Adam(\n",
    "            self.srcnn.parameters(),\n",
    "            0.0001\n",
    "        )\n",
    "\n",
    "        # функция потерь MSE\n",
    "        self.pixel_criterion = nn.MSELoss().to(self.device)\n",
    "\n",
    "        # разрешение hr изображения в формате (h, w)\n",
    "        self.size = (480, 856)\n",
    "        self.crop = (384, 384)\n",
    "\n",
    "        # аугментации для обучения и валидации\n",
    "        train_transform = SameTransform(self.size, 'train', crop=self.crop)\n",
    "\n",
    "        # путь где хранятся папки lr и hr с изображениями\n",
    "        train_prefix = './train_frames'\n",
    "\n",
    "        # train датасет\n",
    "        trainset = SRDataset(\n",
    "            f'{train_prefix}/lr',\n",
    "            f'{train_prefix}/hr',\n",
    "            train_transform\n",
    "        )\n",
    "\n",
    "        # даталоадер для обучения батчами\n",
    "        self.trainloader = DataLoader(\n",
    "            trainset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        # аугментации для инференса\n",
    "        self.resize = transforms.Resize(self.size, antialias=None)\n",
    "        self.np2tensor = transforms.ToTensor()\n",
    "\n",
    "    def train_step(self, lr, hr):\n",
    "        g_hr = self.srcnn(lr)\n",
    "        loss = self.pixel_criterion(g_hr, hr)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self):\n",
    "        self.srcnn.train()\n",
    "        step = 0\n",
    "\n",
    "        while True:\n",
    "            if step >= self.n_steps:\n",
    "                break\n",
    "\n",
    "            for batch in self.trainloader:\n",
    "                lr, hr = batch\n",
    "                lr = lr.to(self.device, non_blocking=True)\n",
    "                hr = hr.to(self.device, non_blocking=True)\n",
    "\n",
    "                mse = self.train_step(lr, hr)\n",
    "                step += 1\n",
    "\n",
    "                if step % self.print_interval == 0:\n",
    "                    print(f'STEP={step} MSE={mse:.5f}')\n",
    "\n",
    "    def frame2tensor(self, img):\n",
    "        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        hr = self.resize(self.np2tensor(rgb))\n",
    "        return hr\n",
    "\n",
    "    def tensor2frame(self, img):\n",
    "        nparr = (img.detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        nparr = np.transpose(nparr, (1, 2, 0))\n",
    "        bgr = cv2.cvtColor(nparr, cv2.COLOR_RGB2BGR)\n",
    "        return bgr\n",
    "\n",
    "    def super_resolution(self, input_video, output_video):\n",
    "        self.srcnn.eval()\n",
    "\n",
    "        cap = cv2.VideoCapture(input_video)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(\n",
    "            output_video,\n",
    "            fourcc,\n",
    "            fps,\n",
    "            (self.size[1], self.size[0])\n",
    "        )\n",
    "\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            tensor = self.frame2tensor(frame).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                output_tensor = self.srcnn(tensor)\n",
    "            output_frame = self.tensor2frame(output_tensor)\n",
    "\n",
    "            writer.write(output_frame)\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
