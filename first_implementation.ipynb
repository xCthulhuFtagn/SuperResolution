{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSRCNN(nn.Module):\n",
    "    def __init__(self, scale_factor, num_channels=1, d=56, s=12, m=4):\n",
    "        super(FSRCNN, self).__init__()\n",
    "        # feature extraction\n",
    "        self.first_part = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, d, kernel_size=5, padding=5//2),\n",
    "            nn.PReLU(d)\n",
    "        )\n",
    "        # shrinking\n",
    "        self.mid_part = [nn.Conv2d(d, s, kernel_size=1), nn.PReLU(s)]\n",
    "        # mapping\n",
    "        for _ in range(m):\n",
    "            self.mid_part.extend([nn.Conv2d(s, s, kernel_size=3, padding=3//2), nn.PReLU(s)])\n",
    "        # expanding\n",
    "        self.mid_part.extend([nn.Conv2d(s, d, kernel_size=1), nn.PReLU(d)])\n",
    "        self.mid_part = nn.Sequential(*self.mid_part)\n",
    "        # Deconvolution\n",
    "        # originally found d instead of s in picture\n",
    "        self.last_part = nn.ConvTranspose2d(d, num_channels, kernel_size=9, stride=scale_factor, padding=9//2,\n",
    "                                            output_padding=scale_factor-1)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.first_part:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "        for m in self.mid_part:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=math.sqrt(2/(m.out_channels*m.weight.data[0][0].numel())))\n",
    "                nn.init.zeros_(m.bias.data)\n",
    "        nn.init.normal_(self.last_part.weight.data, mean=0.0, std=0.001)\n",
    "        nn.init.zeros_(self.last_part.bias.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_part(x)\n",
    "        x = self.mid_part(x)\n",
    "        x = self.last_part(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_path = './rutube_hackaton_super_resolution_khabarovsk/train'\n",
    "# train_path = './train_frames'\n",
    "\n",
    "# lr_path = os.path.join(train_path, 'lr')\n",
    "# hr_path = os.path.join(train_path, 'hr')\n",
    "\n",
    "# if not os.path.exists(train_path):\n",
    "#     os.system(f'mkdir -p {train_path}')\n",
    "\n",
    "# if not os.path.exists(lr_path):\n",
    "#     os.system(f'mkdir -p {lr_path}')\n",
    "\n",
    "# if not os.path.exists(hr_path):\n",
    "#     os.system(f'mkdir -p {hr_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = os.listdir(video_path)\n",
    "# pairs = []\n",
    "# for f in files:\n",
    "#     if f.endswith('_144.mp4'):\n",
    "#         hr_name = f.split('_')[0] + '_480.mp4'\n",
    "#         pairs += [(f, hr_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_frames = 5000\n",
    "# size = int(n_frames // len(pairs))\n",
    "\n",
    "# save_idx = 0\n",
    "# for idx in tqdm(range(len(pairs))):\n",
    "#     pair = pairs[idx]\n",
    "\n",
    "#     lr = os.path.join(video_path, pair[0])\n",
    "#     hr = os.path.join(video_path, pair[1])\n",
    "\n",
    "#     lr_cap = cv2.VideoCapture(lr)\n",
    "#     hr_cap = cv2.VideoCapture(hr)\n",
    "\n",
    "#     lr_len = int(lr_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     hr_len = int(hr_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "#     assert lr_len == hr_len\n",
    "\n",
    "#     frames_idx = [i for i in range(lr_len)]\n",
    "#     if size:\n",
    "#         frames_idx = np.random.choice(frames_idx, size=size, replace=False)\n",
    "\n",
    "#     tmp_idx = 0\n",
    "#     while True:\n",
    "#         success_lr, frame_lr = lr_cap.read()\n",
    "#         success_hr, frame_hr = hr_cap.read()\n",
    "#         if not success_lr or not success_hr:\n",
    "#             break\n",
    "#         if tmp_idx in frames_idx:\n",
    "#             lr_save_path = os.path.join(lr_path, f'{save_idx}.jpg')\n",
    "#             hr_save_path = os.path.join(hr_path, f'{save_idx}.jpg')\n",
    "#             cv2.imwrite(lr_save_path, frame_lr)\n",
    "#             cv2.imwrite(hr_save_path, frame_hr)\n",
    "#             save_idx += 1\n",
    "#         tmp_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    def __init__(self, lr_path, hr_path, transform = None):\n",
    "        self.lr = [os.path.join(lr_path, f) for f in os.listdir(lr_path)]\n",
    "        self.hr = [os.path.join(hr_path, f) for f in os.listdir(hr_path)]\n",
    "        self.lr, self.hr = sorted(self.lr), sorted(self.hr)\n",
    "        assert len(self.lr) == len(self.hr)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr)\n",
    "\n",
    "    def file2np(self, path):\n",
    "        img = cv2.imread(path)\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr = self.file2np(self.lr[idx])\n",
    "        hr = self.file2np(self.hr[idx])\n",
    "        if self.transform is not None: lr, hr = self.transform(lr, hr)\n",
    "        return lr, hr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SameTransform(object):\n",
    "    def __init__(self, mode, crop=None):\n",
    "        self.np2tensor = transforms.ToTensor()\n",
    "        self.mode = mode\n",
    "        self.crop = crop\n",
    "        self.lr_resize = transforms.Resize((214, 120))\n",
    "\n",
    "    def __call__(self, lr, hr):\n",
    "        lr = self.np2tensor(lr)#self.lr_crop.forward(self.np2tensor(lr))\n",
    "        hr = self.np2tensor(hr)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            lr, hr = self.same_transform(lr, hr)\n",
    "            lr = self.np2tensor(self.lr_resize(lr))\n",
    "\n",
    "        if self.crop:\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(lr, self.crop)\n",
    "            lr = TF.crop(lr, i, j, h, w)\n",
    "            hr = TF.crop(hr, i, j, h, w)\n",
    "            \n",
    "        return lr, hr#np.expand_dims(lr, 0), np.expand_dims(hr, 0)\n",
    "    \n",
    "    # после преобразований lr и hr сохраняют пространственное соотношение\n",
    "    def same_transform(self, image1, image2, p=0.5):\n",
    "        if random.random() > p:\n",
    "            image1 = TF.hflip(image1)\n",
    "            image2 = TF.hflip(image2)\n",
    "\n",
    "        if random.random() > p:\n",
    "            image1 = TF.vflip(image1)\n",
    "            image2 = TF.vflip(image2)\n",
    "\n",
    "        return image1, image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self):\n",
    "        # устройство для обучения\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # количество шагов обучения\n",
    "        self.n_steps = 10000\n",
    "\n",
    "        # раз в сколько шагов выводить результаты\n",
    "        self.print_interval = 25\n",
    "\n",
    "        # раз в сколько шагов сохранять чекпоинт\n",
    "        self.save_interval = 2500\n",
    "\n",
    "        self.batch_size =24\n",
    "        self.workers = 8\n",
    "\n",
    "        # инициализация модели\n",
    "        self.fsrcnn = FSRCNN(scale_factor=4, num_channels=3).to(self.device)\n",
    "\n",
    "        # конфигурация оптимизатора Adam\n",
    "        self.optimizer = Adam(\n",
    "            self.fsrcnn.parameters(),\n",
    "            0.0001\n",
    "        )\n",
    "\n",
    "        # функция потерь MSE\n",
    "        self.pixel_criterion = nn.MSELoss().to(self.device)\n",
    "\n",
    "        # разрешение hr изображения в формате (h, w)\n",
    "        self.size = (480, 856)\n",
    "        self.gcrop = transforms.CenterCrop([480, 856])\n",
    "\n",
    "        # # аугментации для обучения и валидации\n",
    "        train_transform = SameTransform('train')\n",
    "\n",
    "        # путь где хранятся папки lr и hr с изображениями\n",
    "        train_prefix = './train_frames'\n",
    "\n",
    "        # train датасет\n",
    "        trainset = SRDataset(\n",
    "            f'{train_prefix}/lr',\n",
    "            f'{train_prefix}/hr',\n",
    "            train_transform\n",
    "        )\n",
    "\n",
    "        # даталоадер для обучения батчами\n",
    "        self.trainloader = DataLoader(\n",
    "            trainset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        # аугментации для инференса\n",
    "        self.resize = transforms.Resize(self.size, antialias=None)\n",
    "        self.np2tensor = transforms.ToTensor()\n",
    "\n",
    "    def train_step(self, lr, hr):\n",
    "        g_hr = self.fsrcnn(lr)\n",
    "        g_hr = self.gcrop.forward(g_hr)\n",
    "        loss = self.pixel_criterion(g_hr, hr)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self):\n",
    "        self.fsrcnn.train()\n",
    "        step = 0\n",
    "\n",
    "        while True:\n",
    "            if step >= self.n_steps:\n",
    "                break\n",
    "\n",
    "            for batch in self.trainloader:\n",
    "                lr, hr = batch\n",
    "                lr = lr.to(self.device, non_blocking=True)\n",
    "                hr = hr.to(self.device, non_blocking=True)\n",
    "\n",
    "                mse = self.train_step(lr, hr)\n",
    "                step += 1\n",
    "\n",
    "                if step % self.print_interval == 0:\n",
    "                    print(f'STEP={step} MSE={mse:.5f}')\n",
    "\n",
    "    def frame2tensor(self, img):\n",
    "        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        hr = self.np2tensor(rgb)#self.resize(self.np2tensor(rgb))\n",
    "        return hr\n",
    "\n",
    "    def tensor2frame(self, img):\n",
    "        nparr = (img.detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        nparr = np.transpose(nparr, (1, 2, 0))\n",
    "        bgr = cv2.cvtColor(nparr, cv2.COLOR_RGB2BGR)\n",
    "        return bgr\n",
    "\n",
    "\n",
    "    def super_resolution(self, input_video, output_video):\n",
    "        crop = transforms.CenterCrop(self.size)\n",
    "        self.fsrcnn.eval()\n",
    "\n",
    "        cap = cv2.VideoCapture(input_video)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(\n",
    "            output_video,\n",
    "            fourcc,\n",
    "            fps,\n",
    "            (self.size[1], self.size[0])\n",
    "        )\n",
    "\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            tensor = self.frame2tensor(frame).to(self.device).unsqueeze_(0)#lr_crop.forward(self.frame2tensor(frame).to(self.device)).unsqueeze_(0)\n",
    "            with torch.no_grad():\n",
    "                output_tensor = self.fsrcnn(tensor)\n",
    "            output_frame = self.tensor2frame(crop.forward(output_tensor[0]))\n",
    "\n",
    "            writer.write(output_frame)\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем объект - trainer для запуска процесса обучения и инференса\n",
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "Caught error in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/owner/Documents/DEV/Python/SuperResolution/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/owner/Documents/DEV/Python/SuperResolution/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/owner/Documents/DEV/Python/SuperResolution/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_8372/452283049.py\", line 19, in __getitem__\n    if self.transform is not None: lr, hr = self.transform(lr, hr)\n  File \"/tmp/ipykernel_8372/1691589499.py\", line 14, in __call__\n    lr = self.np2tensor(cv2.resize(lr,(214,120)))\ncv2.error: OpenCV(4.9.0) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# запускаем процесс обучения\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 78\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainloader:\n\u001b[1;32m     79\u001b[0m     lr, hr \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     80\u001b[0m     lr \u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/DEV/Python/SuperResolution/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/DEV/Python/SuperResolution/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/DEV/Python/SuperResolution/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documents/DEV/Python/SuperResolution/.venv/lib/python3.10/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31merror\u001b[0m: Caught error in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/owner/Documents/DEV/Python/SuperResolution/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/owner/Documents/DEV/Python/SuperResolution/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/owner/Documents/DEV/Python/SuperResolution/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_8372/452283049.py\", line 19, in __getitem__\n    if self.transform is not None: lr, hr = self.transform(lr, hr)\n  File \"/tmp/ipykernel_8372/1691589499.py\", line 14, in __call__\n    lr = self.np2tensor(cv2.resize(lr,(214,120)))\ncv2.error: OpenCV(4.9.0) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n\n"
     ]
    }
   ],
   "source": [
    "# запускаем процесс обучения\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_video = '/home/owner/Documents/DEV/Python/SuperResolution/rutube_hackaton_super_resolution_khabarovsk/train/1_144.mp4'\n",
    "hr_video = '/home/owner/Documents/DEV/Python/SuperResolution/rutube_hackaton_super_resolution_khabarovsk/train/1_480_new.mp4'\n",
    "\n",
    "trainer.super_resolution(lr_video, hr_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oops\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(lr_video)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi',fourcc, 25, (224,120))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        b = cv2.resize(frame,(224,120))\n",
    "        out.write(b)\n",
    "    else:\n",
    "        print(\"oops\")\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
