{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Базовое решение кейса \"Улучшение качества видео - super resolution\" \n",
    "### Кейсодержатель: RUTUBE\n",
    "#### Описание решения: \n",
    "Задача Super Resolution (SR) - повышение разрешения изображений / видео с сохранением качества контента.\n",
    "\n",
    "Приведенное базовое решение основано на алгоритмическом повышении разрешения при помощи интерполяции и улучшении качества  изображения нейронной сетью.\n",
    "\n",
    "Однако данное решение не является единственным, существует большое количество разнообразных подходов, которые показывают лучшее качество на данной задаче. Про существующие методы решения задачи SR вы можете прочитать здесь: https://blog.paperspace.com/image-super-resolution/. \n",
    "\n",
    "Про baseline модель вы можете подробнее прочитать тут: https://arxiv.org/pdf/1501.00092.pdf.\n",
    "\n",
    "![Baseline модель](SRCNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# фиксируем seed\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция инициализации весов модели\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, 0, 0.01)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статья про SRCNN - https://arxiv.org/pdf/1501.00092.pdf.\n",
    "\n",
    "Данная архитектура не делает upsample, upsampling производится на стадии предобработки - при помощи интерполяции изображение низкого разрешения переводится в высокое разрешение, модель старается улучшить качество данного интерполированного изображения.\n",
    "\n",
    "Модель состоит из трех сверточных слоев, для обучения используется функция потерь MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, padding=(9 // 2))\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding=(5 // 2))\n",
    "        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, padding=(5 // 2))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели SRCNN происходит покадрово, поэтому выберем для обучения 5000 кадров случайным образом из 1000 видео (по 5 кадров из каждого видео)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим все необходимые папки, train_path - путь куда сохранятся кадры, video_path - путь к папке с исходными видео."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_path = 'path'\n",
    "# train_path = './train_frames'\n",
    "\n",
    "# lr_path = os.path.join(train_path, 'lr')\n",
    "# hr_path = os.path.join(train_path, 'hr')\n",
    "\n",
    "# if not os.path.exists(train_path):\n",
    "#     os.system(f'mkdir -p {train_path}')\n",
    "\n",
    "# if not os.path.exists(lr_path):\n",
    "#     os.system(f'mkdir -p {lr_path}')\n",
    "\n",
    "# if not os.path.exists(hr_path):\n",
    "#     os.system(f'mkdir -p {hr_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = os.listdir(video_path)\n",
    "# pairs = []\n",
    "# for f in files:\n",
    "#     if f.endswith('_144.mp4'):\n",
    "#         hr_name = f.split('_')[0] + '_480.mp4'\n",
    "#         pairs += [(f, hr_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_frames = 5000\n",
    "# size = int(n_frames // len(pairs))\n",
    "\n",
    "# save_idx = 0\n",
    "# for idx in tqdm(range(len(pairs))):\n",
    "#     pair = pairs[idx]\n",
    "\n",
    "#     lr = os.path.join(video_path, pair[0])\n",
    "#     hr = os.path.join(video_path, pair[1])\n",
    "\n",
    "#     lr_cap = cv2.VideoCapture(lr)\n",
    "#     hr_cap = cv2.VideoCapture(hr)\n",
    "\n",
    "#     lr_len = int(lr_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     hr_len = int(hr_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "#     assert lr_len == hr_len\n",
    "\n",
    "#     frames_idx = [i for i in range(lr_len)]\n",
    "#     if size:\n",
    "#         frames_idx = np.random.choice(frames_idx, size=size, replace=False)\n",
    "\n",
    "#     tmp_idx = 0\n",
    "#     while True:\n",
    "#         success_lr, frame_lr = lr_cap.read()\n",
    "#         success_hr, frame_hr = hr_cap.read()\n",
    "#         if not success_lr or not success_hr:\n",
    "#             break\n",
    "#         if tmp_idx in frames_idx:\n",
    "#             lr_save_path = os.path.join(lr_path, f'{save_idx}.jpg')\n",
    "#             hr_save_path = os.path.join(hr_path, f'{save_idx}.jpg')\n",
    "#             cv2.imwrite(lr_save_path, frame_lr)\n",
    "#             cv2.imwrite(hr_save_path, frame_hr)\n",
    "#             save_idx += 1\n",
    "#         tmp_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный класс формирует датасет для обучения / валидации и тестирования.\n",
    "\n",
    "Структура датасета: корневая папка -> папки train / val / test -> в каждой папке train / val / test лежит 2 папки lr и hr, внутри папок лежат изображения в низком и высоком разрешениях соответственно. Названия файлов в папке lr и hr должны совпадать, например lr/frame1.jpg и hr/frame1.jpg будет использоваться как одно изображение в разных разрешениях для обучения модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    def __init__(self, lr_path, hr_path, transform):\n",
    "        self.lr = [os.path.join(lr_path, f) for f in os.listdir(lr_path)]\n",
    "        self.hr = [os.path.join(hr_path, f) for f in os.listdir(hr_path)]\n",
    "        self.lr, self.hr = sorted(self.lr), sorted(self.hr)\n",
    "        assert len(self.lr) == len(self.hr)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr)\n",
    "\n",
    "    def file2np(self, path):\n",
    "        img = cv2.imread(path)\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr = self.file2np(self.lr[idx])\n",
    "        hr = self.file2np(self.hr[idx])\n",
    "        lr, hr = self.transform(lr, hr)\n",
    "        return lr, hr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аугментации ниже используются для получения torch.FloatTensor с нужными размерами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# после преобразований lr и hr сохраняют пространственное соотношение\n",
    "def same_transform(image1, image2, p=0.5):\n",
    "    if random.random() > p:\n",
    "        image1 = TF.hflip(image1)\n",
    "        image2 = TF.hflip(image2)\n",
    "\n",
    "    if random.random() > p:\n",
    "        image1 = TF.vflip(image1)\n",
    "        image2 = TF.vflip(image2)\n",
    "\n",
    "    return image1, image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SameTransform(object):\n",
    "    def __init__(self, hr_res, mode, crop=None):\n",
    "        self.np2tensor = transforms.ToTensor()\n",
    "        self.resize_lr = transforms.Resize(hr_res, antialias=None)\n",
    "        self.mode = mode\n",
    "        self.crop = crop\n",
    "\n",
    "    def __call__(self, lr, hr):\n",
    "        lr = self.resize_lr(self.np2tensor(lr))\n",
    "        hr = self.np2tensor(hr)\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            lr, hr = same_transform(lr, hr)\n",
    "\n",
    "        if self.crop:\n",
    "            i, j, h, w = transforms.RandomCrop.get_params(lr, self.crop)\n",
    "            lr = TF.crop(lr, i, j, h, w)\n",
    "            hr = TF.crop(hr, i, j, h, w)\n",
    "            \n",
    "        return lr, hr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self):\n",
    "        # устройство для обучения\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # количество шагов обучения\n",
    "        self.n_steps = 10000\n",
    "\n",
    "        # раз в сколько шагов выводить результаты\n",
    "        self.print_interval = 25\n",
    "\n",
    "        # раз в сколько шагов сохранять чекпоинт\n",
    "        self.save_interval = 2500\n",
    "\n",
    "        self.batch_size = 24\n",
    "        self.workers = 8\n",
    "\n",
    "        # инициализация модели\n",
    "        self.srcnn = SRCNN().to(self.device)\n",
    "        self.srcnn.apply(weights_init)\n",
    "\n",
    "        # конфигурация оптимизатора Adam\n",
    "        self.optimizer = Adam(\n",
    "            self.srcnn.parameters(),\n",
    "            0.0001\n",
    "        )\n",
    "\n",
    "        # функция потерь MSE\n",
    "        self.pixel_criterion = nn.MSELoss().to(self.device)\n",
    "\n",
    "        # разрешение hr изображения в формате (h, w)\n",
    "        self.size = (480, 856)\n",
    "        self.crop = (384, 384)\n",
    "\n",
    "        # аугментации для обучения и валидации\n",
    "        train_transform = SameTransform(self.size, 'train', crop=self.crop)\n",
    "\n",
    "        # путь где хранятся папки lr и hr с изображениями\n",
    "        train_prefix = './train_frames'\n",
    "\n",
    "        # train датасет\n",
    "        trainset = SRDataset(\n",
    "            f'{train_prefix}/lr',\n",
    "            f'{train_prefix}/hr',\n",
    "            train_transform\n",
    "        )\n",
    "\n",
    "        # даталоадер для обучения батчами\n",
    "        self.trainloader = DataLoader(\n",
    "            trainset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        # аугментации для инференса\n",
    "        self.resize = transforms.Resize(self.size, antialias=None)\n",
    "        self.np2tensor = transforms.ToTensor()\n",
    "\n",
    "    def train_step(self, lr, hr):\n",
    "        g_hr = self.srcnn(lr)\n",
    "        loss = self.pixel_criterion(g_hr, hr)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self):\n",
    "        self.srcnn.train()\n",
    "        step = 0\n",
    "\n",
    "        while True:\n",
    "            if step >= self.n_steps:\n",
    "                break\n",
    "\n",
    "            for batch in self.trainloader:\n",
    "                lr, hr = batch\n",
    "                lr = lr.to(self.device, non_blocking=True)\n",
    "                hr = hr.to(self.device, non_blocking=True)\n",
    "\n",
    "                mse = self.train_step(lr, hr)\n",
    "                step += 1\n",
    "\n",
    "                if step % self.print_interval == 0:\n",
    "                    print(f'STEP={step} MSE={mse:.5f}')\n",
    "\n",
    "    def frame2tensor(self, img):\n",
    "        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        hr = self.resize(self.np2tensor(rgb))\n",
    "        return hr\n",
    "\n",
    "    def tensor2frame(self, img):\n",
    "        nparr = (img.detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        nparr = np.transpose(nparr, (1, 2, 0))\n",
    "        bgr = cv2.cvtColor(nparr, cv2.COLOR_RGB2BGR)\n",
    "        return bgr\n",
    "\n",
    "    def super_resolution(self, input_video, output_video):\n",
    "        self.srcnn.eval()\n",
    "\n",
    "        cap = cv2.VideoCapture(input_video)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(\n",
    "            output_video,\n",
    "            fourcc,\n",
    "            fps,\n",
    "            (self.size[1], self.size[0])\n",
    "        )\n",
    "\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            tensor = self.frame2tensor(frame).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                output_tensor = self.srcnn(tensor)\n",
    "            output_frame = self.tensor2frame(output_tensor)\n",
    "\n",
    "            writer.write(output_frame)\n",
    "\n",
    "        cap.release()\n",
    "        writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем объект - trainer для запуска процесса обучения и инференса\n",
    "trainer = Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP=25 MSE=0.01336\n",
      "STEP=50 MSE=0.01081\n",
      "STEP=75 MSE=0.00654\n",
      "STEP=100 MSE=0.00375\n",
      "STEP=125 MSE=0.00365\n",
      "STEP=150 MSE=0.00294\n",
      "STEP=175 MSE=0.00331\n",
      "STEP=200 MSE=0.00240\n",
      "STEP=225 MSE=0.00206\n",
      "STEP=250 MSE=0.00197\n",
      "STEP=275 MSE=0.00197\n",
      "STEP=300 MSE=0.00160\n",
      "STEP=325 MSE=0.00215\n",
      "STEP=350 MSE=0.00149\n",
      "STEP=375 MSE=0.00194\n",
      "STEP=400 MSE=0.00171\n",
      "STEP=425 MSE=0.00155\n",
      "STEP=450 MSE=0.00218\n",
      "STEP=475 MSE=0.00153\n",
      "STEP=500 MSE=0.00165\n",
      "STEP=525 MSE=0.00175\n",
      "STEP=550 MSE=0.00205\n",
      "STEP=575 MSE=0.00256\n",
      "STEP=600 MSE=0.00157\n",
      "STEP=625 MSE=0.00187\n",
      "STEP=650 MSE=0.00180\n",
      "STEP=675 MSE=0.00153\n",
      "STEP=700 MSE=0.00161\n",
      "STEP=725 MSE=0.00158\n",
      "STEP=750 MSE=0.00210\n",
      "STEP=775 MSE=0.00152\n",
      "STEP=800 MSE=0.00159\n",
      "STEP=825 MSE=0.00174\n",
      "STEP=850 MSE=0.00140\n",
      "STEP=875 MSE=0.00165\n",
      "STEP=900 MSE=0.00153\n",
      "STEP=925 MSE=0.00171\n",
      "STEP=950 MSE=0.00211\n",
      "STEP=975 MSE=0.00208\n",
      "STEP=1000 MSE=0.00134\n",
      "STEP=1025 MSE=0.00203\n",
      "STEP=1050 MSE=0.00160\n",
      "STEP=1075 MSE=0.00187\n",
      "STEP=1100 MSE=0.00182\n",
      "STEP=1125 MSE=0.00140\n",
      "STEP=1150 MSE=0.00227\n",
      "STEP=1175 MSE=0.00130\n",
      "STEP=1200 MSE=0.00154\n",
      "STEP=1225 MSE=0.00154\n",
      "STEP=1250 MSE=0.00235\n",
      "STEP=1275 MSE=0.00146\n",
      "STEP=1300 MSE=0.00151\n",
      "STEP=1325 MSE=0.00183\n",
      "STEP=1350 MSE=0.00178\n",
      "STEP=1375 MSE=0.00133\n",
      "STEP=1400 MSE=0.00154\n",
      "STEP=1425 MSE=0.00197\n",
      "STEP=1450 MSE=0.00179\n",
      "STEP=1475 MSE=0.00159\n",
      "STEP=1500 MSE=0.00182\n",
      "STEP=1525 MSE=0.00138\n",
      "STEP=1550 MSE=0.00164\n",
      "STEP=1575 MSE=0.00143\n",
      "STEP=1600 MSE=0.00111\n",
      "STEP=1625 MSE=0.00142\n",
      "STEP=1650 MSE=0.00119\n",
      "STEP=1675 MSE=0.00164\n",
      "STEP=1700 MSE=0.00158\n",
      "STEP=1725 MSE=0.00158\n",
      "STEP=1750 MSE=0.00130\n",
      "STEP=1775 MSE=0.00124\n",
      "STEP=1800 MSE=0.00138\n",
      "STEP=1825 MSE=0.00168\n",
      "STEP=1850 MSE=0.00122\n",
      "STEP=1875 MSE=0.00130\n",
      "STEP=1900 MSE=0.00175\n",
      "STEP=1925 MSE=0.00189\n",
      "STEP=1950 MSE=0.00185\n",
      "STEP=1975 MSE=0.00152\n",
      "STEP=2000 MSE=0.00140\n",
      "STEP=2025 MSE=0.00155\n",
      "STEP=2050 MSE=0.00154\n",
      "STEP=2075 MSE=0.00166\n",
      "STEP=2100 MSE=0.00135\n",
      "STEP=2125 MSE=0.00117\n",
      "STEP=2150 MSE=0.00204\n",
      "STEP=2175 MSE=0.00176\n",
      "STEP=2200 MSE=0.00166\n",
      "STEP=2225 MSE=0.00164\n",
      "STEP=2250 MSE=0.00129\n",
      "STEP=2275 MSE=0.00142\n",
      "STEP=2300 MSE=0.00217\n",
      "STEP=2325 MSE=0.00124\n",
      "STEP=2350 MSE=0.00147\n",
      "STEP=2375 MSE=0.00142\n",
      "STEP=2400 MSE=0.00187\n",
      "STEP=2425 MSE=0.00124\n",
      "STEP=2450 MSE=0.00134\n",
      "STEP=2475 MSE=0.00231\n",
      "STEP=2500 MSE=0.00133\n",
      "STEP=2525 MSE=0.00180\n",
      "STEP=2550 MSE=0.00171\n",
      "STEP=2575 MSE=0.00148\n",
      "STEP=2600 MSE=0.00116\n",
      "STEP=2625 MSE=0.00131\n",
      "STEP=2650 MSE=0.00119\n",
      "STEP=2675 MSE=0.00153\n",
      "STEP=2700 MSE=0.00115\n",
      "STEP=2725 MSE=0.00153\n",
      "STEP=2750 MSE=0.00148\n",
      "STEP=2775 MSE=0.00131\n",
      "STEP=2800 MSE=0.00210\n",
      "STEP=2825 MSE=0.00136\n",
      "STEP=2850 MSE=0.00128\n",
      "STEP=2875 MSE=0.00132\n",
      "STEP=2900 MSE=0.00164\n",
      "STEP=2925 MSE=0.00147\n",
      "STEP=2950 MSE=0.00128\n",
      "STEP=2975 MSE=0.00134\n",
      "STEP=3000 MSE=0.00133\n",
      "STEP=3025 MSE=0.00145\n",
      "STEP=3050 MSE=0.00162\n",
      "STEP=3075 MSE=0.00107\n",
      "STEP=3100 MSE=0.00136\n",
      "STEP=3125 MSE=0.00152\n",
      "STEP=3150 MSE=0.00133\n",
      "STEP=3175 MSE=0.00120\n",
      "STEP=3200 MSE=0.00186\n",
      "STEP=3225 MSE=0.00132\n",
      "STEP=3250 MSE=0.00158\n",
      "STEP=3275 MSE=0.00148\n",
      "STEP=3300 MSE=0.00139\n",
      "STEP=3325 MSE=0.00104\n",
      "STEP=3350 MSE=0.00147\n",
      "STEP=3375 MSE=0.00164\n",
      "STEP=3400 MSE=0.00171\n",
      "STEP=3425 MSE=0.00124\n",
      "STEP=3450 MSE=0.00132\n",
      "STEP=3475 MSE=0.00150\n",
      "STEP=3500 MSE=0.00115\n",
      "STEP=3525 MSE=0.00092\n",
      "STEP=3550 MSE=0.00149\n",
      "STEP=3575 MSE=0.00137\n",
      "STEP=3600 MSE=0.00134\n",
      "STEP=3625 MSE=0.00139\n",
      "STEP=3650 MSE=0.00124\n",
      "STEP=3675 MSE=0.00143\n",
      "STEP=3700 MSE=0.00159\n",
      "STEP=3725 MSE=0.00163\n",
      "STEP=3750 MSE=0.00175\n",
      "STEP=3775 MSE=0.00172\n",
      "STEP=3800 MSE=0.00141\n",
      "STEP=3825 MSE=0.00094\n",
      "STEP=3850 MSE=0.00213\n",
      "STEP=3875 MSE=0.00130\n",
      "STEP=3900 MSE=0.00170\n",
      "STEP=3925 MSE=0.00138\n",
      "STEP=3950 MSE=0.00145\n",
      "STEP=3975 MSE=0.00121\n",
      "STEP=4000 MSE=0.00148\n",
      "STEP=4025 MSE=0.00143\n",
      "STEP=4050 MSE=0.00143\n",
      "STEP=4075 MSE=0.00160\n",
      "STEP=4100 MSE=0.00135\n",
      "STEP=4125 MSE=0.00142\n",
      "STEP=4150 MSE=0.00143\n",
      "STEP=4175 MSE=0.00121\n",
      "STEP=4200 MSE=0.00199\n",
      "STEP=4225 MSE=0.00119\n",
      "STEP=4250 MSE=0.00136\n",
      "STEP=4275 MSE=0.00152\n",
      "STEP=4300 MSE=0.00181\n",
      "STEP=4325 MSE=0.00119\n",
      "STEP=4350 MSE=0.00198\n",
      "STEP=4375 MSE=0.00134\n",
      "STEP=4400 MSE=0.00140\n",
      "STEP=4425 MSE=0.00164\n",
      "STEP=4450 MSE=0.00151\n",
      "STEP=4475 MSE=0.00169\n",
      "STEP=4500 MSE=0.00138\n",
      "STEP=4525 MSE=0.00156\n",
      "STEP=4550 MSE=0.00149\n",
      "STEP=4575 MSE=0.00143\n",
      "STEP=4600 MSE=0.00150\n",
      "STEP=4625 MSE=0.00136\n",
      "STEP=4650 MSE=0.00129\n",
      "STEP=4675 MSE=0.00133\n",
      "STEP=4700 MSE=0.00133\n",
      "STEP=4725 MSE=0.00164\n",
      "STEP=4750 MSE=0.00185\n",
      "STEP=4775 MSE=0.00239\n",
      "STEP=4800 MSE=0.00173\n",
      "STEP=4825 MSE=0.00149\n",
      "STEP=4850 MSE=0.00116\n",
      "STEP=4875 MSE=0.00150\n",
      "STEP=4900 MSE=0.00162\n",
      "STEP=4925 MSE=0.00098\n",
      "STEP=4950 MSE=0.00151\n",
      "STEP=4975 MSE=0.00168\n",
      "STEP=5000 MSE=0.00162\n",
      "STEP=5025 MSE=0.00126\n",
      "STEP=5050 MSE=0.00117\n",
      "STEP=5075 MSE=0.00164\n",
      "STEP=5100 MSE=0.00133\n",
      "STEP=5125 MSE=0.00149\n",
      "STEP=5150 MSE=0.00142\n",
      "STEP=5175 MSE=0.00137\n",
      "STEP=5200 MSE=0.00148\n",
      "STEP=5225 MSE=0.00128\n",
      "STEP=5250 MSE=0.00150\n",
      "STEP=5275 MSE=0.00140\n",
      "STEP=5300 MSE=0.00169\n",
      "STEP=5325 MSE=0.00183\n",
      "STEP=5350 MSE=0.00169\n",
      "STEP=5375 MSE=0.00196\n",
      "STEP=5400 MSE=0.00148\n",
      "STEP=5425 MSE=0.00131\n",
      "STEP=5450 MSE=0.00139\n",
      "STEP=5475 MSE=0.00150\n",
      "STEP=5500 MSE=0.00181\n",
      "STEP=5525 MSE=0.00125\n",
      "STEP=5550 MSE=0.00119\n",
      "STEP=5575 MSE=0.00161\n",
      "STEP=5600 MSE=0.00131\n",
      "STEP=5625 MSE=0.00152\n",
      "STEP=5650 MSE=0.00152\n",
      "STEP=5675 MSE=0.00100\n",
      "STEP=5700 MSE=0.00163\n",
      "STEP=5725 MSE=0.00122\n",
      "STEP=5750 MSE=0.00126\n",
      "STEP=5775 MSE=0.00174\n",
      "STEP=5800 MSE=0.00115\n",
      "STEP=5825 MSE=0.00149\n",
      "STEP=5850 MSE=0.00130\n",
      "STEP=5875 MSE=0.00133\n",
      "STEP=5900 MSE=0.00155\n",
      "STEP=5925 MSE=0.00143\n",
      "STEP=5950 MSE=0.00111\n",
      "STEP=5975 MSE=0.00151\n",
      "STEP=6000 MSE=0.00134\n",
      "STEP=6025 MSE=0.00157\n",
      "STEP=6050 MSE=0.00185\n",
      "STEP=6075 MSE=0.00149\n",
      "STEP=6100 MSE=0.00127\n",
      "STEP=6125 MSE=0.00120\n",
      "STEP=6150 MSE=0.00227\n",
      "STEP=6175 MSE=0.00140\n",
      "STEP=6200 MSE=0.00177\n",
      "STEP=6225 MSE=0.00105\n",
      "STEP=6250 MSE=0.00164\n",
      "STEP=6275 MSE=0.00140\n",
      "STEP=6300 MSE=0.00098\n",
      "STEP=6325 MSE=0.00174\n",
      "STEP=6350 MSE=0.00170\n",
      "STEP=6375 MSE=0.00121\n",
      "STEP=6400 MSE=0.00156\n",
      "STEP=6425 MSE=0.00150\n",
      "STEP=6450 MSE=0.00161\n",
      "STEP=6475 MSE=0.00163\n",
      "STEP=6500 MSE=0.00166\n",
      "STEP=6525 MSE=0.00203\n",
      "STEP=6550 MSE=0.00135\n",
      "STEP=6575 MSE=0.00119\n",
      "STEP=6600 MSE=0.00184\n",
      "STEP=6625 MSE=0.00135\n",
      "STEP=6650 MSE=0.00091\n",
      "STEP=6675 MSE=0.00110\n",
      "STEP=6700 MSE=0.00123\n",
      "STEP=6725 MSE=0.00150\n",
      "STEP=6750 MSE=0.00183\n",
      "STEP=6775 MSE=0.00132\n",
      "STEP=6800 MSE=0.00156\n",
      "STEP=6825 MSE=0.00194\n",
      "STEP=6850 MSE=0.00139\n",
      "STEP=6875 MSE=0.00137\n",
      "STEP=6900 MSE=0.00188\n",
      "STEP=6925 MSE=0.00145\n",
      "STEP=6950 MSE=0.00101\n",
      "STEP=6975 MSE=0.00148\n",
      "STEP=7000 MSE=0.00142\n",
      "STEP=7025 MSE=0.00141\n",
      "STEP=7050 MSE=0.00196\n",
      "STEP=7075 MSE=0.00156\n",
      "STEP=7100 MSE=0.00122\n",
      "STEP=7125 MSE=0.00111\n",
      "STEP=7150 MSE=0.00179\n",
      "STEP=7175 MSE=0.00157\n",
      "STEP=7200 MSE=0.00138\n",
      "STEP=7225 MSE=0.00158\n",
      "STEP=7250 MSE=0.00185\n",
      "STEP=7275 MSE=0.00162\n",
      "STEP=7300 MSE=0.00113\n",
      "STEP=7325 MSE=0.00106\n",
      "STEP=7350 MSE=0.00105\n",
      "STEP=7375 MSE=0.00126\n",
      "STEP=7400 MSE=0.00153\n",
      "STEP=7425 MSE=0.00175\n",
      "STEP=7450 MSE=0.00118\n",
      "STEP=7475 MSE=0.00100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# запускаем процесс обучения\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 83\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m lr \u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     81\u001b[0m hr \u001b[38;5;241m=\u001b[39m hr\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 83\u001b[0m mse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[17], line 68\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self, lr, hr)\u001b[0m\n\u001b[1;32m     66\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# запускаем процесс обучения\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инференс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаем путь к видео низкого разрешения, которое лежит у нас на диске (lr_video) и путь к выходному видео, обработанному моделью в высоком разрешении (hr_video)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_video = '/home/owner/Documents/DEV/Python/SuperResolution/rutube_hackaton_super_resolution_khabarovsk/train/1_144.mp4'\n",
    "hr_video = '/home/owner/Documents/DEV/Python/SuperResolution/rutube_hackaton_super_resolution_khabarovsk/train/1_480_newmp4'\n",
    "\n",
    "trainer.super_resolution(lr_video, hr_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
